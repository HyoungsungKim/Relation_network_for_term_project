{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omniglot_train_few_shot_experiement as ot\n",
    "import task_generator_no_args as tg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "python : 3.8.3 (default, May 17 2020, 18:15:42) \n[GCC 10.1.0]\npytorch : 1.4.0\n"
    }
   ],
   "source": [
    "import sys\n",
    "print(f'python : {sys.version}')\n",
    "print(f'pytorch : {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_DIM = 64  \n",
    "RELATION_DIM = 8  \n",
    "CLASS_NUM = 5  \n",
    "SAMPLE_NUM_PER_CLASS = 5  \n",
    "BATCH_NUM_PER_CLASS = 15  \n",
    "EPISODE = 1000000  \n",
    "TEST_EPISODE = 1000  \n",
    "LEARNING_RATE = 0.001  \n",
    "HIDDEN_UNIT = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test just 1 iteration to know how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "init data folders\n"
    }
   ],
   "source": [
    "# * Step 1: init data folders\n",
    "print(\"init data folders\")\n",
    "\n",
    "# * Init character folders for dataset construction\n",
    "metatrain_character_folders, metatest_character_folders = tg.omniglot_character_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "init neural networks\n"
    }
   ],
   "source": [
    "# * Step 2: init neural networks\n",
    "print(\"init neural networks\")\n",
    "\n",
    "feature_encoder = ot.CNNEncoder()\n",
    "relation_network = ot.RelationNetwork(FEATURE_DIM, RELATION_DIM)\n",
    "\n",
    "\n",
    "feature_encoder.apply(ot.weights_init)\n",
    "relation_network.apply(ot.weights_init)\n",
    "\n",
    "feature_encoder.to(device)\n",
    "relation_network.to(device)\n",
    "    \n",
    "feature_encoder_optim = torch.optim.Adam(feature_encoder.parameters(), lr=LEARNING_RATE)\n",
    "#feature_encoder_scheduler = ot.StepLR(feature_encoder_optim, step_size=100000, gamma=0.5)\n",
    "\n",
    "relation_network_optim = torch.optim.Adam(relation_network.parameters(), lr=LEARNING_RATE)\n",
    "#relation_network_scheduler = ot.StepLR(relation_network_optim, step_size=100000, gamma=0.5)\n",
    "\n",
    "if os.path.exists(str(\"./models/omniglot_feature_encoder_\" + str(CLASS_NUM) + \"way_\" + str(SAMPLE_NUM_PER_CLASS) + \"shot.pkl\")):\n",
    "    feature_encoder.load_state_dict(torch.load(str(\"./models/omniglot_feature_encoder_\" + str(CLASS_NUM) + \"way_\" + str(SAMPLE_NUM_PER_CLASS) + \"shot.pkl\")))\n",
    "    print(\"load feature encoder success\")\n",
    "if os.path.exists(str(\"./models/omniglot_relation_network_\" + str(CLASS_NUM) + \"way_\" + str(SAMPLE_NUM_PER_CLASS) + \"shot.pkl\")):\n",
    "    relation_network.load_state_dict(torch.load(str(\"./models/omniglot_relation_network_\" + str(CLASS_NUM) + \"way_\" + str(SAMPLE_NUM_PER_CLASS) + \"shot.pkl\")))\n",
    "    print(\"load relation network success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 1\n",
    "#feature_encoder_scheduler.step(episode)\n",
    "#relation_network_scheduler.step(episode)\n",
    "\n",
    "# * init dataset\n",
    "# * sample_dataloader is to obtain previous samples for compare\n",
    "# * batch_dataloader is to batch samples for training\n",
    "degrees = random.choice([0, 90, 180, 270])\n",
    "task = tg.OmniglotTask(metatrain_character_folders, CLASS_NUM, SAMPLE_NUM_PER_CLASS, BATCH_NUM_PER_CLASS)\n",
    "sample_dataloader = tg.get_data_loader(task, num_per_class=SAMPLE_NUM_PER_CLASS, split=\"train\", shuffle=False, rotation=degrees)\n",
    "batch_dataloader = tg.get_data_loader(task, num_per_class=BATCH_NUM_PER_CLASS, split=\"test\", shuffle=True, rotation=degrees)\n",
    "\n",
    "# * sample datas\n",
    "samples, sample_labels = next(iter(sample_dataloader))  # sample_dataloader.__iter__().next()\n",
    "batches, batch_labels = next(iter(batch_dataloader))  # batch_dataloader.__iter__().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(150, 4)\ntorch.Size([25, 1, 28, 28])\n(150,)\ntorch.Size([25])\n"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(X.shape)\n",
    "print(samples.shape)\n",
    "\n",
    "print(y.shape)\n",
    "print(sample_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(25, 784)\n"
    }
   ],
   "source": [
    "X = np.array(samples)\n",
    "X = X.reshape(25, -1)\n",
    "print(X.shape)\n",
    "Y = np.array(sample_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4,\n       4, 4, 4])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(75, 1, 28, 28)\n(75, 784)\n"
    }
   ],
   "source": [
    "test = np.array(batches)\n",
    "print(test.shape)\n",
    "test = test.reshape(75, -1)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 3, 3, 3, 3, 1, 0, 2, 0, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 3, 0,\n       1, 0, 1, 4, 4, 3, 0, 2, 0, 0, 2, 3, 0, 3, 2, 1, 0, 2, 2, 0, 1, 0,\n       3, 4, 2, 1, 2, 0, 0, 0, 2, 2, 1, 1, 0, 1, 3, 3, 1, 1, 0, 2, 0, 1,\n       1, 3, 2, 2, 0, 1, 0, 3, 1])"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1, 2, 3, 0, 0, 1, 4, 2, 4, 1, 0, 1, 0, 1, 1, 1, 0, 3, 1, 3, 3, 2, 3, 4,\n        3, 4, 0, 2, 2, 4, 4, 2, 3, 3, 2, 0, 0, 1, 4, 4, 3, 2, 0, 3, 3, 4, 2, 0,\n        2, 0, 3, 2, 4, 0, 0, 1, 2, 3, 3, 0, 4, 1, 2, 4, 2, 1, 1, 4, 2, 4, 0, 1,\n        4, 3, 1])\n"
    }
   ],
   "source": [
    "print(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.37333333333333335"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predict = clf.predict(test)\n",
    "true = np.array(batch_labels)\n",
    "assert predict.shape == true.shape\n",
    "\n",
    "accuracy_score(predict, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(0)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f2e6ef957f0>"
     },
     "metadata": {},
     "execution_count": 15
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 251.565 248.518125 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \nL 244.365 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p9735c62b6e)\">\n    <image height=\"218\" id=\"image09f4d38e99\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAABHNCSVQICAgIfAhkiAAAAvBJREFUeJzt3VFqwzAUAMG69P5XVn/zUxsatIqtmRMoCcuDPGQfY4zxBUz1vfoAsAOhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBoGf1Qe4q+M4Vh9hijHG6iM8kokGAaFBQGgQEBoEhAYBoUFAaBCwR/vDu3uyT95HnX22J3/ulUw0CAgNAkKDgNAgIDQICA0CQoPAtnu0nfdFdz77XZloEBAaBIQGAaFBQGgQEBoEhAaBbfdoV7ukpz63kTVMNAgIDQJCg4DQICA0CAgNAtv+vf+uq7//XUXhlYkGAaFBQGgQEBoEhAYBoUFAaBA4hoXPv7xzjcZXvh8TDQJCg4DQICA0CAgNAkKDgNAgYI82yczH1fnJ7sdEg4DQICA0CAgNAkKDgNAgIDQIeK7jJGe7Lq+E2o+JBgGhQUBoEBAaBIQGAaFBQGgQsEeb5GxX5j7Zfkw0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAq7JLHD1uLmrazReCXU/JhoEhAYBoUFAaBAQGgSEBgGhQcAe7QN5rdPzmGgQEBoEhAYBoUFAaBAQGgSEBgF7tEnc6+KViQYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAaBX0cdN7qFTOuFAAAAAElFTkSuQmCC\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m8920972d42\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m8920972d42\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m8920972d42\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m8920972d42\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m8920972d42\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m8920972d42\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m8920972d42\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mcddf36d863\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcddf36d863\" y=\"11.082857\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcddf36d863\" y=\"49.911429\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcddf36d863\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcddf36d863\" y=\"127.568571\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcddf36d863\" y=\"166.397143\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcddf36d863\" y=\"205.225714\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 224.64 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p9735c62b6e\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "print(sample_labels[0])\n",
    "plt.imshow(samples[0].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, sample_labels = samples.to(device), sample_labels.to(device)\n",
    "batches, batch_labels = batches.to(device), batch_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([25, 1, 28, 28])\ntensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n        4], device='cuda:0')\n"
    }
   ],
   "source": [
    "print(samples.shape)\n",
    "print(sample_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([75, 1, 28, 28])\ntensor([1, 2, 3, 0, 0, 1, 4, 2, 4, 1, 0, 1, 0, 1, 1, 1, 0, 3, 1, 3, 3, 2, 3, 4,\n        3, 4, 0, 2, 2, 4, 4, 2, 3, 3, 2, 0, 0, 1, 4, 4, 3, 2, 0, 3, 3, 4, 2, 0,\n        2, 0, 3, 2, 4, 0, 0, 1, 2, 3, 3, 0, 4, 1, 2, 4, 2, 1, 1, 4, 2, 4, 0, 1,\n        4, 3, 1], device='cuda:0')\n"
    }
   ],
   "source": [
    "print(batches.shape)\n",
    "print(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataloader = tg.get_data_loader(task, num_per_class=SAMPLE_NUM_PER_CLASS, split=\"train\", shuffle=False, rotation=degrees)\n",
    "test_dataloader = tg.get_data_loader(task, num_per_class=SAMPLE_NUM_PER_CLASS, split=\"test\", shuffle=True, rotation=degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images, sample_labels = next(iter(sample_dataloader))\n",
    "test_images, test_labels = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([25, 1, 28, 28])\ntensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n        4])\ntorch.Size([25, 1, 28, 28])\ntensor([0, 2, 4, 1, 3, 1, 3, 2, 3, 4, 4, 3, 1, 2, 4, 1, 4, 2, 0, 0, 0, 2, 0, 3,\n        1])\n"
    }
   ],
   "source": [
    "print(sample_images.shape)\n",
    "print(sample_labels)\n",
    "print(test_images.shape)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([25, 64, 5, 5])\n"
    }
   ],
   "source": [
    "# * calculates features\n",
    "linear, sample_features = feature_encoder(samples)\n",
    "print(sample_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 1.]])\n"
    }
   ],
   "source": [
    "one_hot_labels = torch.zeros(SAMPLE_NUM_PER_CLASS * CLASS_NUM, CLASS_NUM).scatter_(1, sample_labels.view(-1, 1), 1)\n",
    "print(one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(0.7738, device='cuda:0', grad_fn=<MeanBackward0>)\n"
    }
   ],
   "source": [
    "print(nn.MSELoss()(one_hot_labels.to(device), linear.view(-1, CLASS_NUM)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([5, 5, 64, 5, 5])\n"
    }
   ],
   "source": [
    "sample_features = sample_features.view(CLASS_NUM, SAMPLE_NUM_PER_CLASS, FEATURE_DIM, 5, 5)\n",
    "print(sample_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([25, 1600])\n"
    }
   ],
   "source": [
    "sample_features_temp = sample_features.reshape(25, -1)\n",
    "print(sample_features_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[[0.0000e+00, 1.0318e+00, 6.6294e-01, 7.8039e-01, 9.5128e-01],\n          [1.9493e-03, 0.0000e+00, 5.3378e-01, 0.0000e+00, 0.0000e+00],\n          [6.8132e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0285e-01],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5523e-01],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 8.2598e-02, 0.0000e+00]],\n\n         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 2.3721e-01, 0.0000e+00, 0.0000e+00],\n          [7.7468e-01, 1.4093e+00, 5.0608e-01, 3.3169e-01, 0.0000e+00]],\n\n         [[2.5836e-01, 1.5689e-01, 1.4243e+00, 1.2697e+00, 1.0691e+00],\n          [0.0000e+00, 7.3928e-01, 3.3199e-01, 0.0000e+00, 3.6572e-01],\n          [1.5265e+00, 3.1951e-01, 2.5892e+00, 1.1257e+00, 3.3447e-01],\n          [0.0000e+00, 8.1803e-01, 1.9363e-01, 4.8775e-01, 5.1666e-01],\n          [0.0000e+00, 8.5661e-01, 4.9002e-01, 3.6356e-01, 3.5246e-01]],\n\n         ...,\n\n         [[0.0000e+00, 0.0000e+00, 5.5884e-02, 6.4885e-02, 0.0000e+00],\n          [0.0000e+00, 9.7837e-01, 7.0414e-01, 3.6828e-01, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4588e-02, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 6.5388e-01, 3.9105e-01, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6245e-01]],\n\n         [[4.6075e-01, 0.0000e+00, 0.0000e+00, 3.6978e-01, 2.0696e-01],\n          [6.5542e-02, 0.0000e+00, 1.2349e-01, 0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 5.9201e-01, 8.7850e-01, 0.0000e+00],\n          [9.1413e-01, 1.3562e+00, 4.6927e-03, 8.6925e-01, 6.0340e-01],\n          [3.8619e-01, 4.7497e-01, 8.7108e-01, 0.0000e+00, 0.0000e+00]],\n\n         [[2.9428e-01, 0.0000e+00, 2.5105e-01, 5.4566e-01, 7.3515e-01],\n          [1.7169e+00, 1.0199e+00, 7.2397e-01, 0.0000e+00, 0.0000e+00],\n          [1.3527e+00, 1.4808e+00, 1.1228e+00, 7.3312e-01, 0.0000e+00],\n          [0.0000e+00, 4.2192e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [4.4994e-01, 1.1838e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n\n\n        [[[1.7297e+00, 2.7814e+00, 1.4758e+00, 7.1216e-01, 7.7564e-01],\n          [9.4015e-01, 9.4218e-01, 8.5469e-01, 6.7809e-01, 1.3589e+00],\n          [4.9038e-01, 1.0317e-01, 9.7397e-01, 7.6257e-01, 3.1261e-01],\n          [3.4069e-01, 4.3905e-02, 1.5713e-02, 7.8390e-01, 0.0000e+00],\n          [3.3734e-01, 0.0000e+00, 0.0000e+00, 2.8425e-01, 1.6410e-03]],\n\n         [[4.3352e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [4.5962e-01, 2.8019e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [1.5660e+00, 5.4246e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [1.3489e+00, 0.0000e+00, 3.9874e-01, 0.0000e+00, 0.0000e+00]],\n\n         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5603e-02, 8.8440e-01],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7021e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9843e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3801e-01, 1.3917e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3002e+00]],\n\n         ...,\n\n         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [9.0486e-01, 1.0939e-01, 9.1511e-01, 0.0000e+00, 0.0000e+00],\n          [1.3170e+00, 1.4735e+00, 2.1192e+00, 9.7362e-01, 8.6483e-01],\n          [6.1186e-01, 1.3297e+00, 2.9647e+00, 1.4131e+00, 9.5510e-01],\n          [5.0750e-01, 1.2789e+00, 3.1612e+00, 2.1421e+00, 1.1502e+00]],\n\n         [[2.9613e-01, 5.3031e-01, 5.3583e-01, 0.0000e+00, 2.8700e-02],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8736e-01],\n          [2.8817e-01, 6.5385e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [6.6067e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1759e-01],\n          [2.7856e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [1.0137e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [1.0901e+00, 3.4038e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [1.4964e+00, 1.2495e+00, 1.0481e-01, 0.0000e+00, 0.0000e+00]]],\n\n\n        [[[0.0000e+00, 0.0000e+00, 6.7770e-01, 1.6316e-02, 1.3531e+00],\n          [0.0000e+00, 0.0000e+00, 1.2890e+00, 4.2768e-01, 6.1230e-01],\n          [0.0000e+00, 0.0000e+00, 5.2378e-01, 0.0000e+00, 1.0542e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0498e-01],\n          [4.0230e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n         [[6.4990e-01, 0.0000e+00, 1.0712e-02, 6.5244e-01, 1.5388e+00],\n          [6.1189e-02, 0.0000e+00, 1.3743e+00, 6.8658e-01, 2.8217e-01],\n          [0.0000e+00, 0.0000e+00, 1.6433e+00, 6.5170e-02, 6.3079e-01],\n          [2.0178e-01, 0.0000e+00, 9.6047e-01, 0.0000e+00, 5.9981e-01],\n          [1.1264e-01, 7.6269e-02, 2.9244e-01, 8.3137e-01, 2.0824e-01]],\n\n         [[6.4376e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9998e-01],\n          [5.0269e-01, 0.0000e+00, 7.6661e-01, 7.2236e-01, 7.1061e-01],\n          [6.4563e-01, 9.8737e-01, 1.2937e+00, 4.5642e-01, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 2.3882e-01, 6.4360e-01, 0.0000e+00],\n          [0.0000e+00, 8.9960e-01, 2.5409e+00, 7.1269e-01, 6.0413e-01]],\n\n         ...,\n\n         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [5.0963e-01, 0.0000e+00, 1.1063e+00, 0.0000e+00, 0.0000e+00],\n          [2.4837e-01, 0.0000e+00, 9.0095e-01, 3.1569e-01, 3.5686e-01],\n          [3.9052e-01, 2.1761e-01, 4.3291e-01, 6.8307e-01, 3.6156e-01],\n          [5.0542e-01, 0.0000e+00, 0.0000e+00, 8.6898e-01, 3.0539e-02]],\n\n         [[0.0000e+00, 5.7297e-01, 1.0733e-01, 1.2753e+00, 0.0000e+00],\n          [0.0000e+00, 1.4435e+00, 7.0972e-01, 1.3140e+00, 0.0000e+00],\n          [0.0000e+00, 1.6475e+00, 1.2115e+00, 1.2895e+00, 7.1998e-01],\n          [4.1710e-02, 6.4054e-01, 2.8628e+00, 1.8266e+00, 8.9169e-01],\n          [5.8815e-01, 1.3906e+00, 2.2877e+00, 2.0910e+00, 1.3954e+00]],\n\n         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [6.1773e-01, 1.7616e-01, 4.4680e-02, 0.0000e+00, 0.0000e+00],\n          [9.9254e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [1.2470e-01, 3.0535e-01, 1.8893e-01, 0.0000e+00, 0.0000e+00],\n          [8.9070e-01, 4.6752e-01, 1.0520e+00, 0.0000e+00, 0.0000e+00]]],\n\n\n        [[[2.8009e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8110e-01],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n         [[2.4011e+00, 2.7218e+00, 1.7842e+00, 7.5881e-01, 5.3214e-01],\n          [2.1413e+00, 2.6502e+00, 6.5745e-01, 1.6041e+00, 0.0000e+00],\n          [2.2641e+00, 8.2472e-01, 2.5746e+00, 0.0000e+00, 0.0000e+00],\n          [1.2116e+00, 0.0000e+00, 6.3487e-01, 0.0000e+00, 1.8749e-01],\n          [1.2330e+00, 7.6050e-01, 0.0000e+00, 5.1837e-02, 8.8897e-01]],\n\n         [[0.0000e+00, 1.2878e+00, 1.8245e+00, 1.7133e+00, 5.8066e-01],\n          [3.7835e-01, 1.2266e+00, 0.0000e+00, 8.2071e-01, 6.4275e-01],\n          [8.6629e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8209e-01],\n          [1.1746e+00, 1.4395e+00, 7.8582e-02, 4.3014e-01, 1.1783e+00],\n          [6.5258e-01, 2.0615e-01, 4.7979e-01, 1.8333e+00, 3.8745e-01]],\n\n         ...,\n\n         [[0.0000e+00, 0.0000e+00, 2.6967e+00, 2.2773e+00, 1.6126e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9662e+00, 1.8818e+00],\n          [0.0000e+00, 0.0000e+00, 2.3383e+00, 1.6762e+00, 2.6192e+00],\n          [2.0993e-01, 4.2026e-01, 2.0048e+00, 2.1257e+00, 2.9399e+00],\n          [7.8663e-01, 0.0000e+00, 1.5628e+00, 1.6227e+00, 9.8465e-01]],\n\n         [[3.8259e-01, 3.1844e-01, 1.1655e+00, 3.5715e-01, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0219e-01, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.2270e-01],\n          [0.0000e+00, 0.0000e+00, 1.3144e+00, 0.0000e+00, 1.5065e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5349e-02, 2.1270e+00]],\n\n         [[1.2005e+00, 0.0000e+00, 1.2008e-01, 0.0000e+00, 0.0000e+00],\n          [2.1968e+00, 0.0000e+00, 0.0000e+00, 5.5579e-01, 0.0000e+00],\n          [2.0995e+00, 4.1970e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [2.1596e+00, 5.7640e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [7.9243e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n\n\n        [[[3.3970e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4565e-01],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [4.9937e-01, 0.0000e+00, 0.0000e+00, 1.3623e+00, 6.6043e-01],\n          [0.0000e+00, 0.0000e+00, 7.0188e-02, 3.3583e-01, 0.0000e+00]],\n\n         [[6.5661e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8662e-01],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1154e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6279e-02]],\n\n         [[0.0000e+00, 0.0000e+00, 4.0284e-01, 0.0000e+00, 1.9172e-01],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8322e-01],\n          [2.2147e-01, 8.2033e-01, 0.0000e+00, 4.4262e-01, 6.0398e-01]],\n\n         ...,\n\n         [[0.0000e+00, 0.0000e+00, 2.9707e-01, 1.5524e+00, 1.1477e+00],\n          [0.0000e+00, 7.5891e-02, 1.1806e+00, 1.7347e+00, 1.9037e+00],\n          [0.0000e+00, 1.4962e+00, 5.7541e-01, 2.8787e-01, 7.2833e-01],\n          [0.0000e+00, 2.5406e-01, 7.9376e-01, 7.2365e-01, 4.4158e-01],\n          [5.8917e-01, 1.2064e+00, 1.6498e+00, 8.5247e-01, 5.1323e-01]],\n\n         [[3.4849e-01, 4.3778e-01, 1.5077e-01, 1.7731e-01, 0.0000e+00],\n          [4.7493e-01, 5.5117e-01, 0.0000e+00, 0.0000e+00, 1.5505e-01],\n          [6.6743e-01, 0.0000e+00, 3.4568e-01, 7.5331e-01, 5.8121e-01],\n          [2.2808e-01, 6.3875e-01, 3.9020e-01, 1.9643e+00, 8.8564e-01],\n          [0.0000e+00, 0.0000e+00, 8.7416e-01, 1.3197e+00, 0.0000e+00]],\n\n         [[6.8952e-01, 1.1757e+00, 1.6334e+00, 8.0252e-01, 1.9529e-01],\n          [1.6703e+00, 0.0000e+00, 7.5846e-01, 1.1033e-01, 5.9358e-01],\n          [1.0888e+00, 0.0000e+00, 5.8103e-01, 0.0000e+00, 0.0000e+00],\n          [7.1751e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n          [3.3490e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]]],\n       device='cuda:0', grad_fn=<SelectBackward>)\n"
    }
   ],
   "source": [
    "print(sample_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0.0000, 1.0318, 0.6629,  ..., 0.0000, 0.0000, 0.0000], device='cuda:0',\n       grad_fn=<SelectBackward>)\ntensor([[[[[0.0000e+00, 1.0318e+00, 6.6294e-01, 7.8039e-01, 9.5128e-01],\n           [1.9493e-03, 0.0000e+00, 5.3378e-01, 0.0000e+00, 0.0000e+00],\n           [6.8132e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0285e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5523e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 8.2598e-02, 0.0000e+00]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 2.3721e-01, 0.0000e+00, 0.0000e+00],\n           [7.7468e-01, 1.4093e+00, 5.0608e-01, 3.3169e-01, 0.0000e+00]],\n\n          [[2.5836e-01, 1.5689e-01, 1.4243e+00, 1.2697e+00, 1.0691e+00],\n           [0.0000e+00, 7.3928e-01, 3.3199e-01, 0.0000e+00, 3.6572e-01],\n           [1.5265e+00, 3.1951e-01, 2.5892e+00, 1.1257e+00, 3.3447e-01],\n           [0.0000e+00, 8.1803e-01, 1.9363e-01, 4.8775e-01, 5.1666e-01],\n           [0.0000e+00, 8.5661e-01, 4.9002e-01, 3.6356e-01, 3.5246e-01]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 5.5884e-02, 6.4885e-02, 0.0000e+00],\n           [0.0000e+00, 9.7837e-01, 7.0414e-01, 3.6828e-01, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4588e-02, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 6.5388e-01, 3.9105e-01, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6245e-01]],\n\n          [[4.6075e-01, 0.0000e+00, 0.0000e+00, 3.6978e-01, 2.0696e-01],\n           [6.5542e-02, 0.0000e+00, 1.2349e-01, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 5.9201e-01, 8.7850e-01, 0.0000e+00],\n           [9.1413e-01, 1.3562e+00, 4.6927e-03, 8.6925e-01, 6.0340e-01],\n           [3.8619e-01, 4.7497e-01, 8.7108e-01, 0.0000e+00, 0.0000e+00]],\n\n          [[2.9428e-01, 0.0000e+00, 2.5105e-01, 5.4566e-01, 7.3515e-01],\n           [1.7169e+00, 1.0199e+00, 7.2397e-01, 0.0000e+00, 0.0000e+00],\n           [1.3527e+00, 1.4808e+00, 1.1228e+00, 7.3312e-01, 0.0000e+00],\n           [0.0000e+00, 4.2192e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [4.4994e-01, 1.1838e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n\n\n         [[[1.7297e+00, 2.7814e+00, 1.4758e+00, 7.1216e-01, 7.7564e-01],\n           [9.4015e-01, 9.4218e-01, 8.5469e-01, 6.7809e-01, 1.3589e+00],\n           [4.9038e-01, 1.0317e-01, 9.7397e-01, 7.6257e-01, 3.1261e-01],\n           [3.4069e-01, 4.3905e-02, 1.5713e-02, 7.8390e-01, 0.0000e+00],\n           [3.3734e-01, 0.0000e+00, 0.0000e+00, 2.8425e-01, 1.6410e-03]],\n\n          [[4.3352e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [4.5962e-01, 2.8019e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.5660e+00, 5.4246e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.3489e+00, 0.0000e+00, 3.9874e-01, 0.0000e+00, 0.0000e+00]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5603e-02, 8.8440e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7021e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9843e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3801e-01, 1.3917e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3002e+00]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [9.0486e-01, 1.0939e-01, 9.1511e-01, 0.0000e+00, 0.0000e+00],\n           [1.3170e+00, 1.4735e+00, 2.1192e+00, 9.7362e-01, 8.6483e-01],\n           [6.1186e-01, 1.3297e+00, 2.9647e+00, 1.4131e+00, 9.5510e-01],\n           [5.0750e-01, 1.2789e+00, 3.1612e+00, 2.1421e+00, 1.1502e+00]],\n\n          [[2.9613e-01, 5.3031e-01, 5.3583e-01, 0.0000e+00, 2.8700e-02],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8736e-01],\n           [2.8817e-01, 6.5385e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [6.6067e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1759e-01],\n           [2.7856e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.0137e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.0901e+00, 3.4038e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.4964e+00, 1.2495e+00, 1.0481e-01, 0.0000e+00, 0.0000e+00]]],\n\n\n         [[[0.0000e+00, 0.0000e+00, 6.7770e-01, 1.6316e-02, 1.3531e+00],\n           [0.0000e+00, 0.0000e+00, 1.2890e+00, 4.2768e-01, 6.1230e-01],\n           [0.0000e+00, 0.0000e+00, 5.2378e-01, 0.0000e+00, 1.0542e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0498e-01],\n           [4.0230e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[6.4990e-01, 0.0000e+00, 1.0712e-02, 6.5244e-01, 1.5388e+00],\n           [6.1189e-02, 0.0000e+00, 1.3743e+00, 6.8658e-01, 2.8217e-01],\n           [0.0000e+00, 0.0000e+00, 1.6433e+00, 6.5170e-02, 6.3079e-01],\n           [2.0178e-01, 0.0000e+00, 9.6047e-01, 0.0000e+00, 5.9981e-01],\n           [1.1264e-01, 7.6269e-02, 2.9244e-01, 8.3137e-01, 2.0824e-01]],\n\n          [[6.4376e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9998e-01],\n           [5.0269e-01, 0.0000e+00, 7.6661e-01, 7.2236e-01, 7.1061e-01],\n           [6.4563e-01, 9.8737e-01, 1.2937e+00, 4.5642e-01, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 2.3882e-01, 6.4360e-01, 0.0000e+00],\n           [0.0000e+00, 8.9960e-01, 2.5409e+00, 7.1269e-01, 6.0413e-01]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [5.0963e-01, 0.0000e+00, 1.1063e+00, 0.0000e+00, 0.0000e+00],\n           [2.4837e-01, 0.0000e+00, 9.0095e-01, 3.1569e-01, 3.5686e-01],\n           [3.9052e-01, 2.1761e-01, 4.3291e-01, 6.8307e-01, 3.6156e-01],\n           [5.0542e-01, 0.0000e+00, 0.0000e+00, 8.6898e-01, 3.0539e-02]],\n\n          [[0.0000e+00, 5.7297e-01, 1.0733e-01, 1.2753e+00, 0.0000e+00],\n           [0.0000e+00, 1.4435e+00, 7.0972e-01, 1.3140e+00, 0.0000e+00],\n           [0.0000e+00, 1.6475e+00, 1.2115e+00, 1.2895e+00, 7.1998e-01],\n           [4.1710e-02, 6.4054e-01, 2.8628e+00, 1.8266e+00, 8.9169e-01],\n           [5.8815e-01, 1.3906e+00, 2.2877e+00, 2.0910e+00, 1.3954e+00]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [6.1773e-01, 1.7616e-01, 4.4680e-02, 0.0000e+00, 0.0000e+00],\n           [9.9254e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.2470e-01, 3.0535e-01, 1.8893e-01, 0.0000e+00, 0.0000e+00],\n           [8.9070e-01, 4.6752e-01, 1.0520e+00, 0.0000e+00, 0.0000e+00]]],\n\n\n         [[[2.8009e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8110e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[2.4011e+00, 2.7218e+00, 1.7842e+00, 7.5881e-01, 5.3214e-01],\n           [2.1413e+00, 2.6502e+00, 6.5745e-01, 1.6041e+00, 0.0000e+00],\n           [2.2641e+00, 8.2472e-01, 2.5746e+00, 0.0000e+00, 0.0000e+00],\n           [1.2116e+00, 0.0000e+00, 6.3487e-01, 0.0000e+00, 1.8749e-01],\n           [1.2330e+00, 7.6050e-01, 0.0000e+00, 5.1837e-02, 8.8897e-01]],\n\n          [[0.0000e+00, 1.2878e+00, 1.8245e+00, 1.7133e+00, 5.8066e-01],\n           [3.7835e-01, 1.2266e+00, 0.0000e+00, 8.2071e-01, 6.4275e-01],\n           [8.6629e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8209e-01],\n           [1.1746e+00, 1.4395e+00, 7.8582e-02, 4.3014e-01, 1.1783e+00],\n           [6.5258e-01, 2.0615e-01, 4.7979e-01, 1.8333e+00, 3.8745e-01]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 2.6967e+00, 2.2773e+00, 1.6126e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9662e+00, 1.8818e+00],\n           [0.0000e+00, 0.0000e+00, 2.3383e+00, 1.6762e+00, 2.6192e+00],\n           [2.0993e-01, 4.2026e-01, 2.0048e+00, 2.1257e+00, 2.9399e+00],\n           [7.8663e-01, 0.0000e+00, 1.5628e+00, 1.6227e+00, 9.8465e-01]],\n\n          [[3.8259e-01, 3.1844e-01, 1.1655e+00, 3.5715e-01, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0219e-01, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.2270e-01],\n           [0.0000e+00, 0.0000e+00, 1.3144e+00, 0.0000e+00, 1.5065e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5349e-02, 2.1270e+00]],\n\n          [[1.2005e+00, 0.0000e+00, 1.2008e-01, 0.0000e+00, 0.0000e+00],\n           [2.1968e+00, 0.0000e+00, 0.0000e+00, 5.5579e-01, 0.0000e+00],\n           [2.0995e+00, 4.1970e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [2.1596e+00, 5.7640e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [7.9243e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n\n\n         [[[3.3970e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4565e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [4.9937e-01, 0.0000e+00, 0.0000e+00, 1.3623e+00, 6.6043e-01],\n           [0.0000e+00, 0.0000e+00, 7.0188e-02, 3.3583e-01, 0.0000e+00]],\n\n          [[6.5661e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8662e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1154e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6279e-02]],\n\n          [[0.0000e+00, 0.0000e+00, 4.0284e-01, 0.0000e+00, 1.9172e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8322e-01],\n           [2.2147e-01, 8.2033e-01, 0.0000e+00, 4.4262e-01, 6.0398e-01]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 2.9707e-01, 1.5524e+00, 1.1477e+00],\n           [0.0000e+00, 7.5891e-02, 1.1806e+00, 1.7347e+00, 1.9037e+00],\n           [0.0000e+00, 1.4962e+00, 5.7541e-01, 2.8787e-01, 7.2833e-01],\n           [0.0000e+00, 2.5406e-01, 7.9376e-01, 7.2365e-01, 4.4158e-01],\n           [5.8917e-01, 1.2064e+00, 1.6498e+00, 8.5247e-01, 5.1323e-01]],\n\n          [[3.4849e-01, 4.3778e-01, 1.5077e-01, 1.7731e-01, 0.0000e+00],\n           [4.7493e-01, 5.5117e-01, 0.0000e+00, 0.0000e+00, 1.5505e-01],\n           [6.6743e-01, 0.0000e+00, 3.4568e-01, 7.5331e-01, 5.8121e-01],\n           [2.2808e-01, 6.3875e-01, 3.9020e-01, 1.9643e+00, 8.8564e-01],\n           [0.0000e+00, 0.0000e+00, 8.7416e-01, 1.3197e+00, 0.0000e+00]],\n\n          [[6.8952e-01, 1.1757e+00, 1.6334e+00, 8.0252e-01, 1.9529e-01],\n           [1.6703e+00, 0.0000e+00, 7.5846e-01, 1.1033e-01, 5.9358e-01],\n           [1.0888e+00, 0.0000e+00, 5.8103e-01, 0.0000e+00, 0.0000e+00],\n           [7.1751e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [3.3490e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]]],\n\n\n\n        [[[[8.6602e-01, 8.6347e-01, 7.1441e-01, 7.2484e-01, 8.7736e-01],\n           [7.3436e-01, 9.0123e-01, 5.0530e-01, 4.8711e-01, 6.0947e-01],\n           [8.3720e-01, 6.7878e-01, 4.0962e-01, 5.1208e-01, 1.3940e+00],\n           [8.3937e-01, 7.3208e-01, 1.4401e-01, 6.5811e-01, 5.3885e-01],\n           [8.7014e-01, 5.1113e-01, 2.3062e-01, 4.4123e-01, 8.3983e-01]],\n\n          [[3.7152e-01, 2.5240e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [5.2013e-01, 0.0000e+00, 0.0000e+00, 6.0041e-01, 0.0000e+00],\n           [8.4646e-02, 0.0000e+00, 0.0000e+00, 1.0084e-01, 1.0401e-01],\n           [0.0000e+00, 5.1635e-02, 4.7269e-01, 4.0078e-01, 1.0664e-01],\n           [8.2840e-02, 4.8541e-01, 7.7002e-02, 6.3458e-02, 2.5117e-01]],\n\n          [[7.9856e-03, 0.0000e+00, 2.8429e-01, 6.3660e-01, 0.0000e+00],\n           [0.0000e+00, 2.9061e-01, 3.5685e-01, 4.4167e-01, 1.1836e+00],\n           [4.2366e-01, 8.5061e-01, 2.0852e-01, 0.0000e+00, 0.0000e+00],\n           [5.4013e-01, 2.3727e-01, 5.0313e-01, 0.0000e+00, 2.9489e-01],\n           [4.2438e-01, 7.8839e-01, 5.9456e-01, 5.3044e-01, 4.5969e-01]],\n\n          ...,\n\n          [[0.0000e+00, 3.1682e-01, 5.0313e-01, 4.4379e-01, 0.0000e+00],\n           [0.0000e+00, 2.1793e-02, 4.7476e-01, 0.0000e+00, 2.1380e-01],\n           [0.0000e+00, 2.0835e-01, 1.3397e-01, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 3.4618e-01, 1.9997e-01, 0.0000e+00, 5.3056e-01]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7676e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4885e-01, 4.4694e-01],\n           [0.0000e+00, 8.6047e-02, 1.1132e+00, 4.7575e-01, 1.0438e+00],\n           [0.0000e+00, 5.5619e-01, 2.8852e-03, 2.2293e-01, 0.0000e+00]],\n\n          [[3.6179e-01, 3.0531e-01, 6.4754e-01, 4.2344e-01, 4.8516e-01],\n           [1.1643e+00, 5.6371e-01, 8.2622e-01, 4.4663e-01, 0.0000e+00],\n           [9.9838e-01, 9.1034e-01, 8.4942e-01, 1.0961e+00, 3.9506e-01],\n           [8.1631e-01, 4.9571e-01, 3.1829e-01, 6.8144e-03, 4.0231e-02],\n           [5.9621e-01, 1.0314e-01, 3.3310e-01, 4.8988e-01, 6.1109e-01]]],\n\n\n         [[[4.5779e-01, 3.3340e-01, 7.3920e-01, 1.5912e+00, 1.4602e+00],\n           [5.3671e-01, 1.8261e+00, 6.3231e-03, 3.7267e-01, 5.1687e-01],\n           [4.3114e-01, 1.5383e+00, 6.4654e-01, 2.6632e-04, 7.9693e-01],\n           [4.2414e-01, 0.0000e+00, 1.8467e-01, 0.0000e+00, 3.3187e-01],\n           [2.2100e-01, 0.0000e+00, 0.0000e+00, 3.0361e-01, 1.1451e-01]],\n\n          [[5.2786e-01, 1.2771e+00, 1.8613e+00, 7.4111e-01, 2.3903e-01],\n           [6.2450e-01, 1.7784e+00, 3.9356e-01, 1.6740e-01, 1.2067e+00],\n           [1.0793e+00, 1.2009e+00, 1.3910e+00, 0.0000e+00, 1.0241e-02],\n           [3.0788e-01, 2.2476e-02, 0.0000e+00, 8.1813e-01, 0.0000e+00],\n           [8.5468e-01, 0.0000e+00, 1.5937e-01, 0.0000e+00, 0.0000e+00]],\n\n          [[5.4774e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4535e+00],\n           [1.4378e-01, 7.0214e-01, 0.0000e+00, 0.0000e+00, 3.9152e-03],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8042e+00, 1.1675e+00]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 2.0877e-01, 1.1832e-02, 1.2258e+00],\n           [0.0000e+00, 7.2709e-02, 4.8443e-01, 8.4168e-01, 0.0000e+00],\n           [0.0000e+00, 7.2826e-02, 3.8203e-01, 1.5495e+00, 5.1691e-01]],\n\n          [[3.3038e-01, 7.4302e-01, 9.3833e-01, 0.0000e+00, 3.8519e-01],\n           [0.0000e+00, 9.1069e-01, 6.3538e-01, 2.1761e-01, 4.2850e-01],\n           [5.4141e-01, 6.2249e-01, 1.4151e+00, 0.0000e+00, 0.0000e+00],\n           [8.1026e-01, 2.5439e+00, 0.0000e+00, 9.8471e-01, 0.0000e+00],\n           [1.1199e+00, 1.9159e+00, 2.0403e+00, 7.7932e-01, 1.0877e+00]],\n\n          [[5.3095e-01, 4.6931e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [6.0595e-01, 1.8530e-01, 6.1806e-01, 0.0000e+00, 0.0000e+00],\n           [1.3624e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.3908e+00, 0.0000e+00, 1.3694e-01, 0.0000e+00, 0.0000e+00],\n           [6.7399e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n\n\n         [[[2.6472e-01, 4.2500e-01, 2.7736e-01, 3.2723e-01, 4.1013e-01],\n           [4.2024e-01, 4.4472e-01, 2.4577e-02, 6.4899e-01, 9.6420e-01],\n           [5.2614e-01, 3.8479e-01, 5.2433e-01, 1.3257e-01, 6.6227e-01],\n           [2.1152e-01, 5.8917e-01, 2.6813e-02, 2.9236e-01, 3.1626e-01],\n           [1.3847e-01, 2.6622e-01, 0.0000e+00, 1.5687e-01, 5.0184e-01]],\n\n          [[2.3721e-01, 3.0642e-01, 3.6709e-01, 5.1943e-01, 5.5388e-01],\n           [1.4175e-01, 1.8667e-02, 2.6429e-01, 3.2633e-01, 5.5235e-01],\n           [7.4496e-02, 5.2091e-01, 0.0000e+00, 8.8127e-01, 7.7647e-01],\n           [9.6415e-02, 3.0277e-01, 4.3508e-01, 6.8051e-01, 5.2931e-01],\n           [1.1152e-01, 1.7958e-01, 5.0112e-01, 2.3564e-01, 7.2624e-01]],\n\n          [[0.0000e+00, 3.1484e-01, 1.9568e-01, 4.2539e-01, 1.9746e-01],\n           [3.0088e-01, 0.0000e+00, 0.0000e+00, 4.7230e-01, 7.5387e-01],\n           [3.3302e-01, 8.0221e-01, 4.0838e-01, 0.0000e+00, 2.1523e-01],\n           [2.1361e-01, 7.3821e-01, 4.2166e-01, 4.1877e-01, 0.0000e+00],\n           [4.2709e-01, 2.9114e-01, 2.4202e-01, 6.6411e-01, 1.1109e-01]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 3.3737e-01, 1.6011e-01, 0.0000e+00]],\n\n          [[0.0000e+00, 1.4262e-01, 0.0000e+00, 0.0000e+00, 3.9291e-02],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 9.7625e-03, 2.9971e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8140e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4764e-01]],\n\n          [[3.7917e-01, 5.8862e-01, 0.0000e+00, 5.0736e-02, 4.3862e-01],\n           [4.0501e-01, 1.5238e-01, 5.9089e-01, 1.9699e-02, 0.0000e+00],\n           [6.4214e-01, 4.5928e-01, 4.4083e-01, 1.8196e-01, 0.0000e+00],\n           [2.1490e-01, 5.1026e-01, 1.3103e-01, 4.2339e-01, 6.6436e-02],\n           [5.9645e-01, 8.2211e-02, 3.6415e-01, 5.2478e-02, 3.2419e-01]]],\n\n\n         [[[1.1831e+00, 1.4471e+00, 1.4657e+00, 1.5585e+00, 7.8030e-01],\n           [1.3256e+00, 1.6460e+00, 1.4701e+00, 6.9984e-01, 4.9362e-01],\n           [1.1147e+00, 2.0418e+00, 1.0627e+00, 5.3528e-01, 1.5772e+00],\n           [1.1858e+00, 1.2585e+00, 3.3874e-01, 1.0843e+00, 5.2155e-01],\n           [6.0278e-01, 1.8303e-01, 5.3695e-01, 4.0271e-01, 4.8890e-01]],\n\n          [[5.1927e-01, 9.5845e-01, 3.3971e-01, 3.9325e-02, 0.0000e+00],\n           [8.7811e-01, 9.6730e-01, 2.9678e-01, 4.4185e-01, 4.1489e-01],\n           [3.6942e-01, 7.6975e-01, 2.6917e-01, 2.8379e-01, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3157e-01],\n           [4.0772e-01, 4.3826e-01, 0.0000e+00, 1.8968e-01, 5.7567e-02]],\n\n          [[3.7594e-01, 7.4829e-01, 8.1543e-01, 0.0000e+00, 0.0000e+00],\n           [1.2494e+00, 1.8186e+00, 2.7968e-01, 0.0000e+00, 5.7258e-01],\n           [1.1036e+00, 1.1152e+00, 7.8101e-01, 0.0000e+00, 7.1731e-01],\n           [0.0000e+00, 9.6811e-01, 0.0000e+00, 6.8301e-01, 0.0000e+00],\n           [2.9908e-01, 1.4123e+00, 0.0000e+00, 0.0000e+00, 7.7346e-01]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 2.2526e-01, 0.0000e+00, 1.2041e-01],\n           [0.0000e+00, 0.0000e+00, 4.1438e-01, 7.7159e-01, 3.2340e-01],\n           [0.0000e+00, 4.3855e-01, 3.8142e-01, 0.0000e+00, 5.2828e-01],\n           [3.9671e-01, 1.4399e+00, 1.5504e+00, 7.4635e-01, 2.6414e-01],\n           [6.5232e-01, 1.0223e+00, 1.5248e+00, 4.4227e-01, 8.3466e-01]],\n\n          [[5.5102e-01, 3.3459e-01, 8.2800e-02, 2.4449e-01, 6.5768e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 4.9938e-01, 1.8386e-01],\n           [0.0000e+00, 0.0000e+00, 2.1986e-01, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 3.0251e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [7.8260e-01, 4.4862e-01, 1.2374e-01, 0.0000e+00, 0.0000e+00]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [2.8549e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 7.7901e-01, 0.0000e+00],\n           [6.6098e-01, 0.0000e+00, 0.0000e+00, 6.7249e-01, 0.0000e+00]]],\n\n\n         [[[8.0660e-01, 6.1365e-01, 7.2831e-01, 6.1716e-01, 6.4355e-01],\n           [9.7020e-01, 1.1683e+00, 8.8182e-01, 8.9264e-01, 5.8281e-01],\n           [9.9283e-01, 1.1494e+00, 6.8363e-01, 1.6678e-01, 5.5367e-01],\n           [6.0867e-01, 5.7151e-01, 1.8471e-01, 5.5839e-01, 1.0591e+00],\n           [5.4577e-01, 3.7546e-01, 2.7015e-01, 2.4073e-01, 4.6361e-02]],\n\n          [[8.6894e-01, 5.5635e-01, 7.9354e-01, 2.9426e-01, 3.3259e-01],\n           [9.2363e-01, 1.1874e+00, 1.4488e+00, 1.0710e+00, 1.2291e+00],\n           [9.2216e-01, 6.3828e-01, 7.3960e-01, 1.3453e+00, 8.4157e-01],\n           [1.1508e+00, 6.2371e-01, 8.1760e-01, 1.3499e+00, 1.8517e+00],\n           [8.7356e-01, 3.4026e-01, 9.6977e-01, 4.6091e-01, 9.3346e-01]],\n\n          [[0.0000e+00, 2.3047e-01, 2.1849e-01, 2.0636e-01, 7.6639e-01],\n           [6.9308e-03, 2.9099e-01, 2.9542e-01, 4.3111e-01, 4.7409e-01],\n           [2.2987e-01, 5.8014e-01, 0.0000e+00, 5.6057e-01, 6.3514e-01],\n           [2.8787e-01, 8.0679e-01, 7.9666e-01, 4.2867e-02, 5.7892e-01],\n           [5.1958e-01, 5.5049e-01, 5.7664e-01, 1.2334e-03, 0.0000e+00]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 1.0073e-01, 1.6759e-02, 1.6053e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4682e-02, 5.3299e-02],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8341e-01, 7.8370e-01],\n           [0.0000e+00, 0.0000e+00, 2.4013e-01, 0.0000e+00, 0.0000e+00]],\n\n          [[3.2110e-01, 2.8024e-01, 0.0000e+00, 2.9576e-01, 1.0373e-02],\n           [0.0000e+00, 0.0000e+00, 1.6380e-01, 0.0000e+00, 5.0867e-01],\n           [0.0000e+00, 3.5390e-01, 5.1670e-01, 4.3677e-02, 1.7659e-01],\n           [7.2520e-02, 4.3707e-01, 0.0000e+00, 0.0000e+00, 2.0729e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.2901e-01]],\n\n          [[3.4536e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [6.1423e-01, 4.2465e-01, 4.9738e-01, 0.0000e+00, 0.0000e+00],\n           [3.9630e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [5.8883e-01, 0.0000e+00, 3.5393e-02, 3.5296e-01, 0.0000e+00],\n           [8.9060e-01, 4.9093e-01, 2.3052e-01, 1.5986e-01, 3.6580e-01]]]],\n\n\n\n        [[[[3.6106e-01, 2.7208e-01, 8.2316e-01, 6.1755e-01, 8.4196e-01],\n           [0.0000e+00, 6.0213e-01, 5.9023e-01, 8.1909e-01, 8.1466e-01],\n           [0.0000e+00, 1.8802e-01, 2.1404e-01, 0.0000e+00, 1.2494e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.1044e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[3.2027e-01, 0.0000e+00, 1.7974e-01, 0.0000e+00, 2.4941e-01],\n           [2.4672e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7340e-01],\n           [4.4136e-01, 8.8579e-01, 1.4125e+00, 4.1331e-02, 4.5883e-01],\n           [6.4754e-01, 0.0000e+00, 2.1325e-01, 0.0000e+00, 0.0000e+00],\n           [3.5900e-01, 3.4607e-01, 0.0000e+00, 8.4472e-01, 0.0000e+00]],\n\n          [[0.0000e+00, 1.7251e-01, 1.7821e-01, 0.0000e+00, 5.0421e-01],\n           [2.5779e-01, 8.3352e-01, 1.3526e+00, 7.3243e-01, 1.0036e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5345e-01, 7.5423e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8520e-01]],\n\n          ...,\n\n          [[6.6578e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [2.7445e-01, 2.2604e-01, 9.0751e-01, 4.9074e-01, 1.2290e-01],\n           [3.3176e-01, 2.2906e-01, 8.6361e-01, 1.5746e+00, 1.5903e-01],\n           [0.0000e+00, 3.1647e-01, 1.2342e+00, 1.6575e+00, 6.0101e-01],\n           [6.7719e-01, 5.8874e-01, 1.3974e+00, 1.2118e+00, 4.8457e-01]],\n\n          [[0.0000e+00, 0.0000e+00, 4.5045e-02, 2.1582e-01, 7.6674e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1757e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8191e-01]],\n\n          [[3.8431e-01, 5.8356e-01, 1.2826e+00, 5.2226e-01, 7.7404e-01],\n           [8.4952e-01, 1.4976e+00, 1.1012e+00, 2.0397e+00, 6.6330e-01],\n           [2.9926e-01, 8.7533e-01, 6.3326e-01, 2.8039e-01, 8.0936e-01],\n           [0.0000e+00, 6.7234e-01, 1.2846e+00, 9.9054e-01, 3.3019e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n\n\n         [[[7.6029e-01, 1.0228e+00, 9.6005e-01, 6.8398e-01, 5.9855e-03],\n           [0.0000e+00, 1.8485e-02, 4.9667e-01, 2.4281e-03, 0.0000e+00],\n           [3.7786e-02, 0.0000e+00, 0.0000e+00, 2.1482e-01, 2.7075e-01],\n           [0.0000e+00, 0.0000e+00, 2.0847e-01, 5.1464e-01, 7.0788e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[4.4332e-01, 0.0000e+00, 0.0000e+00, 1.2844e-01, 3.0150e-02],\n           [6.5826e-02, 0.0000e+00, 2.7373e-01, 0.0000e+00, 0.0000e+00],\n           [3.1796e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.6712e+00, 0.0000e+00, 9.2141e-02, 2.3711e-01, 0.0000e+00],\n           [5.9178e-01, 5.0648e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[2.2356e-01, 2.2858e-01, 0.0000e+00, 0.0000e+00, 1.7788e-01],\n           [0.0000e+00, 9.2267e-01, 9.7326e-01, 4.8998e-01, 5.8643e-01],\n           [2.9202e-01, 9.5106e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [4.9543e-01, 2.7616e+00, 2.1390e+00, 3.0645e+00, 8.6035e-01],\n           [7.2761e-01, 2.1978e+00, 8.0062e-01, 9.9873e-01, 0.0000e+00]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7060e-01, 0.0000e+00],\n           [1.5575e+00, 6.9795e-01, 5.6793e-01, 7.1901e-01, 0.0000e+00],\n           [1.6601e+00, 1.3924e+00, 1.1608e+00, 9.7607e-01, 3.1867e-01],\n           [8.5966e-01, 2.9188e+00, 2.3653e+00, 2.2771e+00, 6.1316e-01],\n           [1.1914e+00, 1.9449e+00, 5.3653e-01, 4.8142e-01, 6.8695e-01]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8398e-01, 9.1598e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7380e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[5.1099e-01, 8.1140e-01, 1.2122e+00, 6.2458e-01, 4.8554e-01],\n           [9.1361e-01, 5.6605e-01, 6.2101e-01, 7.3080e-03, 0.0000e+00],\n           [6.1320e-01, 0.0000e+00, 1.4720e+00, 1.8986e+00, 0.0000e+00],\n           [9.4283e-01, 1.5739e+00, 1.0570e+00, 2.9305e-01, 3.9667e-01],\n           [0.0000e+00, 0.0000e+00, 8.3363e-01, 1.2734e+00, 5.4432e-01]]],\n\n\n         [[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2014e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 6.7768e-01, 1.2175e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8782e-01, 8.2223e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0381e-01, 8.0911e-01]],\n\n          [[7.9332e-01, 2.7586e-01, 5.2629e-01, 6.3555e-01, 0.0000e+00],\n           [1.3464e+00, 1.5446e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.9004e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [5.2392e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.6204e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[1.0495e+00, 7.9018e-01, 0.0000e+00, 0.0000e+00, 3.2261e-01],\n           [1.0607e+00, 1.0158e+00, 2.4956e+00, 1.1831e+00, 8.9097e-01],\n           [6.4733e-01, 9.6969e-01, 4.0881e-01, 0.0000e+00, 2.5785e-01],\n           [0.0000e+00, 2.2082e+00, 1.6357e+00, 1.3927e+00, 4.2642e-01],\n           [3.7642e-01, 2.5564e+00, 1.7231e+00, 2.0491e+00, 1.2664e+00]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 9.6575e-01, 1.4247e+00, 2.9547e-01],\n           [0.0000e+00, 7.0912e-01, 1.0918e+00, 7.9826e-01, 1.8162e+00],\n           [1.0153e+00, 1.6485e+00, 1.4178e+00, 3.7407e-01, 8.7300e-01],\n           [1.4760e+00, 1.6798e+00, 1.7461e+00, 2.4424e+00, 2.2753e+00],\n           [1.1286e+00, 2.6451e+00, 2.1913e+00, 2.2231e+00, 1.4681e+00]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6134e-01, 5.3470e-02],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [7.9160e-01, 5.2062e-01, 9.3139e-01, 1.2090e+00, 2.1757e-01]],\n\n          [[1.6013e-01, 0.0000e+00, 1.0155e+00, 3.5254e-01, 0.0000e+00],\n           [6.1452e-01, 0.0000e+00, 0.0000e+00, 7.3909e-01, 0.0000e+00],\n           [5.0229e-01, 0.0000e+00, 1.1140e+00, 0.0000e+00, 0.0000e+00],\n           [2.3157e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.0606e+00, 5.6898e-01, 4.7437e-02, 0.0000e+00, 6.8184e-02]]],\n\n\n         [[[2.9514e-01, 1.6667e+00, 8.0906e-01, 9.8339e-01, 1.6161e-01],\n           [4.1364e-01, 3.9024e-01, 0.0000e+00, 6.4084e-01, 0.0000e+00],\n           [2.3831e-03, 1.0995e+00, 0.0000e+00, 6.7159e-01, 1.8881e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3624e-01, 3.8226e-01]],\n\n          [[7.7144e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9190e-01],\n           [8.3838e-01, 0.0000e+00, 0.0000e+00, 9.6081e-02, 0.0000e+00],\n           [9.0810e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.2831e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [3.2566e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2724e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [7.6370e-02, 0.0000e+00, 1.2781e+00, 2.7216e-01, 1.4067e-01]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 2.8747e-02, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 3.4619e-01, 0.0000e+00, 8.2214e-01],\n           [5.6724e-01, 9.4844e-01, 6.0990e-01, 0.0000e+00, 3.9478e-01]],\n\n          [[7.1278e-01, 1.7615e-01, 6.8473e-01, 6.1649e-03, 0.0000e+00],\n           [7.6106e-01, 1.4377e-01, 2.9268e-01, 1.1113e+00, 0.0000e+00],\n           [1.3273e+00, 3.4435e-01, 1.4420e+00, 1.3960e+00, 0.0000e+00],\n           [1.6561e+00, 1.2786e+00, 1.4787e+00, 1.3420e+00, 0.0000e+00],\n           [2.0559e+00, 2.9183e+00, 2.7618e+00, 2.5461e+00, 1.4188e+00]],\n\n          [[6.4261e-01, 3.2457e-01, 6.7131e-01, 6.7959e-01, 2.5844e-01],\n           [9.2805e-01, 1.2399e+00, 1.1906e+00, 1.6195e+00, 5.8639e-02],\n           [1.6386e-01, 5.5743e-01, 0.0000e+00, 6.4163e-01, 4.0055e-01],\n           [6.9040e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [3.4330e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0246e-02]]],\n\n\n         [[[1.3675e+00, 1.2020e+00, 6.7676e-01, 3.9947e-01, 5.4605e-01],\n           [0.0000e+00, 1.7513e-01, 0.0000e+00, 9.3426e-02, 1.0787e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0930e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5733e-02]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2468e-01, 7.9717e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2311e-01, 2.3464e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [3.9987e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [7.4655e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[3.7205e-01, 0.0000e+00, 0.0000e+00, 1.1870e-03, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5053e-01, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[0.0000e+00, 0.0000e+00, 4.3924e-02, 0.0000e+00, 5.7214e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [9.4550e-01, 3.4473e-01, 1.0705e+00, 2.9941e-02, 4.0600e-01]],\n\n          [[1.0241e+00, 3.9438e-01, 2.7998e-01, 6.3658e-03, 7.5366e-02],\n           [6.8553e-01, 1.2971e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 7.7243e-02, 0.0000e+00, 0.0000e+00, 1.6681e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]]],\n\n\n\n        [[[[6.0306e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5252e-01],\n           [0.0000e+00, 5.0298e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.3258e-01, 8.4935e-01, 7.0888e-02, 0.0000e+00, 3.5369e-01],\n           [5.4583e-01, 3.4993e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [4.9778e-01, 7.4965e-01, 7.1227e-01, 1.1064e+00, 6.4148e-01]],\n\n          [[2.7824e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0846e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4501e-01, 0.0000e+00],\n           [6.0422e-02, 2.8104e-01, 0.0000e+00, 1.8651e-01, 5.7973e-01]],\n\n          [[1.0754e-01, 3.8692e-02, 2.6016e-01, 0.0000e+00, 0.0000e+00],\n           [1.0099e+00, 1.3257e+00, 5.7503e-01, 0.0000e+00, 3.2533e-01],\n           [1.0846e+00, 1.0690e+00, 9.4499e-01, 3.3732e-01, 0.0000e+00],\n           [9.2702e-01, 0.0000e+00, 8.0022e-01, 6.6486e-02, 1.0923e+00],\n           [1.4243e+00, 1.6556e+00, 1.2589e+00, 2.8404e+00, 1.3979e+00]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7783e-02, 0.0000e+00],\n           [4.8800e-01, 0.0000e+00, 2.5418e-01, 3.4803e-01, 0.0000e+00],\n           [0.0000e+00, 1.6478e-01, 7.2624e-01, 7.6319e-01, 7.9105e-01],\n           [0.0000e+00, 1.3369e+00, 2.1495e+00, 9.5072e-01, 5.9279e-01],\n           [4.7728e-02, 1.4172e+00, 1.5748e+00, 9.6819e-01, 6.8309e-01]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3530e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0946e-01, 6.4709e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0773e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1895e-02],\n           [0.0000e+00, 1.4665e-03, 2.1519e-01, 0.0000e+00, 0.0000e+00]],\n\n          [[3.0130e-01, 1.0096e+00, 1.0464e+00, 4.9784e-01, 4.1428e-01],\n           [7.2797e-01, 9.8184e-01, 1.5042e+00, 1.5271e+00, 0.0000e+00],\n           [8.6779e-01, 6.7852e-01, 1.7245e+00, 1.9829e+00, 9.1147e-01],\n           [1.1523e+00, 1.4194e+00, 2.1262e+00, 6.0364e-01, 7.3971e-01],\n           [6.2148e-01, 9.4898e-01, 9.1207e-01, 1.1087e+00, 0.0000e+00]]],\n\n\n         [[[1.9725e+00, 9.8194e-01, 7.6238e-01, 1.7586e+00, 7.3616e-01],\n           [2.2698e+00, 0.0000e+00, 0.0000e+00, 1.2085e+00, 1.4083e+00],\n           [9.2419e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0683e-01],\n           [7.2079e-01, 0.0000e+00, 0.0000e+00, 3.2641e-02, 6.9686e-01],\n           [4.8577e-01, 1.2113e+00, 1.3837e-01, 1.0775e+00, 2.9206e-01]],\n\n          [[1.6807e+00, 6.4585e-01, 6.9876e-01, 3.5672e-01, 6.7479e-01],\n           [2.2681e+00, 8.1263e-01, 6.6079e-01, 0.0000e+00, 0.0000e+00],\n           [1.4766e+00, 9.1694e-01, 6.3995e-01, 0.0000e+00, 1.5546e-01],\n           [9.7423e-01, 4.8696e-01, 8.6460e-01, 1.3809e-01, 2.8907e-01],\n           [4.8691e-01, 6.5300e-01, 0.0000e+00, 5.2890e-01, 2.8319e-01]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4723e-02],\n           [2.4739e-02, 1.4336e-01, 1.0557e+00, 1.0940e+00, 1.2835e+00],\n           [2.1408e-01, 0.0000e+00, 1.1606e+00, 5.6349e-01, 9.6648e-01],\n           [1.5183e-01, 8.8788e-01, 6.8083e-01, 1.1266e+00, 8.3400e-01],\n           [6.1163e-01, 0.0000e+00, 7.8929e-01, 5.8948e-01, 1.3025e+00]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [5.8167e-01, 1.0496e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.5565e+00, 1.2953e+00, 9.2783e-01, 2.3591e-01, 0.0000e+00],\n           [9.3410e-01, 5.2615e-01, 1.1796e+00, 7.2010e-01, 5.3124e-01]],\n\n          [[4.7627e-01, 6.1800e-01, 0.0000e+00, 0.0000e+00, 3.4287e-02],\n           [5.7509e-01, 8.1822e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [6.0159e-01, 4.9824e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [8.2517e-01, 9.7156e-01, 1.0898e+00, 1.2686e-01, 3.2742e-02],\n           [2.6647e-01, 0.0000e+00, 5.0094e-01, 0.0000e+00, 3.0099e-01]],\n\n          [[0.0000e+00, 0.0000e+00, 8.4953e-03, 0.0000e+00, 2.2466e-01],\n           [5.4169e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [5.4340e-01, 8.3949e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [8.0256e-01, 8.6303e-01, 0.0000e+00, 3.1358e-01, 0.0000e+00]]],\n\n\n         [[[7.8724e-01, 5.1027e-01, 7.0546e-01, 9.9898e-01, 6.3530e-01],\n           [4.4091e-01, 1.0121e+00, 4.7722e-01, 3.4927e-02, 1.3088e+00],\n           [7.3766e-01, 7.9445e-01, 7.1586e-02, 0.0000e+00, 1.5190e-01],\n           [8.4904e-01, 6.3508e-02, 0.0000e+00, 2.3042e-01, 1.0878e-01],\n           [4.6388e-01, 4.0123e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[4.6669e-01, 8.1374e-02, 3.8714e-02, 2.6301e-01, 0.0000e+00],\n           [1.3487e-01, 3.4594e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [4.2724e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [5.3481e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [5.2309e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 6.2575e-01, 9.0677e-01],\n           [0.0000e+00, 1.2978e+00, 0.0000e+00, 6.6650e-02, 1.3243e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [2.4171e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.6688e-01, 7.9622e-01, 0.0000e+00, 0.0000e+00, 5.3181e-01]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7371e-01, 2.8352e-01],\n           [0.0000e+00, 0.0000e+00, 2.3636e-01, 2.5057e-01, 8.3857e-01],\n           [0.0000e+00, 3.7120e-01, 6.7272e-01, 1.0354e+00, 5.0077e-01]],\n\n          [[8.6948e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [4.3234e-01, 2.2134e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 4.4835e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.3735e-02, 0.0000e+00, 9.6917e-02, 0.0000e+00, 0.0000e+00]],\n\n          [[2.8863e-01, 0.0000e+00, 0.0000e+00, 5.6809e-01, 1.9123e-01],\n           [2.3190e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.8850e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [5.9685e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n\n\n         [[[6.1319e-01, 8.9730e-01, 8.2903e-01, 5.7546e-01, 5.4072e-01],\n           [2.6730e-01, 1.2811e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [8.6878e-01, 3.0964e-01, 0.0000e+00, 3.2729e-01, 0.0000e+00],\n           [5.8143e-01, 0.0000e+00, 0.0000e+00, 2.5107e-01, 0.0000e+00],\n           [6.7273e-01, 2.9952e-02, 5.5958e-01, 6.2613e-01, 1.2787e-01]],\n\n          [[7.1531e-01, 7.5017e-01, 7.3332e-01, 5.9593e-01, 8.5932e-01],\n           [0.0000e+00, 5.8904e-01, 4.1339e-01, 0.0000e+00, 8.5903e-01],\n           [9.5384e-01, 3.6424e-01, 1.2613e+00, 0.0000e+00, 1.1011e+00],\n           [1.1385e+00, 3.8145e-01, 1.5844e+00, 0.0000e+00, 9.1641e-01],\n           [4.4763e-01, 4.5432e-01, 1.0255e+00, 0.0000e+00, 5.7817e-01]],\n\n          [[7.5000e-01, 9.1711e-01, 3.3857e-01, 0.0000e+00, 9.3679e-02],\n           [1.9059e+00, 1.4675e+00, 1.0333e+00, 0.0000e+00, 6.7708e-01],\n           [7.9161e-01, 1.6508e+00, 2.4603e-04, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 1.0770e+00, 0.0000e+00, 0.0000e+00, 2.5135e-01],\n           [1.2294e+00, 1.0483e+00, 7.8508e-01, 0.0000e+00, 1.3041e-01]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 6.4427e-01, 1.0183e+00, 3.3895e-01],\n           [3.1952e-01, 5.6225e-01, 1.7702e+00, 8.7451e-01, 4.2171e-01],\n           [1.7087e-01, 1.9509e+00, 2.0348e+00, 1.3103e+00, 1.0404e+00],\n           [7.1014e-01, 8.5556e-01, 2.4308e+00, 4.9523e-01, 1.2255e+00],\n           [1.0165e+00, 5.4421e-01, 5.3251e-01, 1.6207e-01, 4.5232e-01]],\n\n          [[0.0000e+00, 0.0000e+00, 1.8994e-01, 4.4291e-01, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 4.6454e-02, 0.0000e+00, 1.3838e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5559e-02, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[3.0935e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.7975e+00, 0.0000e+00, 0.0000e+00, 2.3677e-01, 0.0000e+00],\n           [6.0527e-01, 4.8949e-03, 0.0000e+00, 4.8710e-01, 0.0000e+00],\n           [1.2027e+00, 0.0000e+00, 0.0000e+00, 7.8281e-02, 1.4413e-01],\n           [1.2821e+00, 1.2146e-01, 0.0000e+00, 2.8962e-01, 0.0000e+00]]],\n\n\n         [[[0.0000e+00, 8.5256e-01, 9.2019e-01, 7.6140e-01, 5.0813e-01],\n           [0.0000e+00, 8.0097e-01, 4.8231e-01, 1.7658e+00, 1.4321e+00],\n           [4.1269e-01, 2.1402e-02, 4.0699e-02, 5.3432e-01, 1.0236e+00],\n           [8.8203e-01, 0.0000e+00, 0.0000e+00, 6.7793e-02, 1.0285e+00],\n           [3.8765e-01, 0.0000e+00, 0.0000e+00, 3.0363e-01, 9.3133e-01]],\n\n          [[7.4506e-01, 3.5249e-01, 5.1071e-01, 7.4513e-01, 5.3018e-01],\n           [4.4090e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2468e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [3.5324e-01, 0.0000e+00, 1.2486e-01, 8.7102e-02, 0.0000e+00]],\n\n          [[0.0000e+00, 2.5169e-01, 3.8200e-01, 0.0000e+00, 8.2137e-01],\n           [7.9276e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5768e-01],\n           [6.2468e-01, 1.2382e+00, 0.0000e+00, 7.4482e-01, 9.6277e-01],\n           [0.0000e+00, 4.5465e-01, 2.0395e-01, 4.1098e-02, 5.6383e-01],\n           [3.5898e-01, 4.0845e-01, 1.0360e+00, 7.9283e-01, 1.2099e+00]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5209e-03],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1573e-01],\n           [3.5939e-02, 8.2665e-01, 0.0000e+00, 1.9866e-01, 3.4222e-01],\n           [0.0000e+00, 6.6099e-01, 1.6506e-01, 0.0000e+00, 5.0813e-01]],\n\n          [[0.0000e+00, 1.1637e+00, 1.3605e+00, 6.6436e-01, 0.0000e+00],\n           [8.6769e-01, 2.0973e-01, 9.7756e-01, 1.9097e+00, 3.0540e-01],\n           [1.0095e+00, 8.0847e-01, 7.2294e-01, 1.6164e+00, 0.0000e+00],\n           [2.8115e-01, 1.0942e+00, 1.0065e+00, 8.3187e-01, 0.0000e+00],\n           [6.0077e-02, 4.4102e-01, 5.3365e-01, 2.2359e-01, 0.0000e+00]]]],\n\n\n\n        [[[[1.1032e-01, 0.0000e+00, 6.0391e-01, 4.6065e-01, 2.3198e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6682e-01]],\n\n          [[1.8097e+00, 7.4562e-01, 7.0280e-01, 0.0000e+00, 4.7031e-01],\n           [2.0183e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9558e-01],\n           [9.7929e-01, 3.3837e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [5.6101e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0034e-01],\n           [4.4240e-01, 3.2013e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[0.0000e+00, 0.0000e+00, 8.2790e-01, 0.0000e+00, 3.3118e-01],\n           [0.0000e+00, 0.0000e+00, 6.5952e-01, 4.9411e-01, 9.8176e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5572e+00, 1.7952e+00],\n           [0.0000e+00, 0.0000e+00, 7.2506e-01, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 8.2706e-01, 1.7964e-01],\n           [2.8843e-01, 0.0000e+00, 0.0000e+00, 1.0534e+00, 4.8587e-01],\n           [0.0000e+00, 2.6501e-01, 7.2164e-01, 1.8242e-01, 1.4845e+00],\n           [4.3466e-01, 7.5898e-01, 4.8526e-01, 2.9953e-01, 1.8625e-01]],\n\n          [[1.4948e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [9.5839e-01, 9.1205e-01, 0.0000e+00, 3.3039e-01, 0.0000e+00],\n           [5.6424e-01, 1.2385e+00, 1.0482e+00, 1.9148e+00, 0.0000e+00],\n           [1.4324e+00, 2.5269e+00, 1.8723e-01, 3.4944e+00, 0.0000e+00]],\n\n          [[9.0231e-01, 1.5825e+00, 1.2127e+00, 0.0000e+00, 2.3712e-01],\n           [8.5879e-01, 2.0636e+00, 1.0448e+00, 5.0743e-01, 0.0000e+00],\n           [1.6305e+00, 1.5639e+00, 5.1474e-01, 1.0990e-01, 5.4087e-01],\n           [1.1292e+00, 5.7954e-01, 6.4052e-01, 0.0000e+00, 0.0000e+00],\n           [1.2760e+00, 8.4523e-01, 0.0000e+00, 0.0000e+00, 1.0111e-01]]],\n\n\n         [[[7.7598e-01, 1.5870e+00, 7.8961e-01, 6.7284e-01, 7.0618e-01],\n           [6.3710e-01, 1.3806e+00, 6.5285e-01, 6.3566e-01, 1.2688e-01],\n           [1.1137e+00, 1.4300e+00, 4.7062e-01, 3.5679e-01, 4.0390e-01],\n           [3.8311e-01, 1.1918e+00, 2.8942e-01, 7.7540e-01, 8.9584e-01],\n           [4.1523e-01, 1.1323e-01, 0.0000e+00, 0.0000e+00, 8.0061e-02]],\n\n          [[6.2576e-01, 5.7798e-01, 1.8413e-01, 0.0000e+00, 1.6819e-01],\n           [1.0058e+00, 7.5329e-01, 0.0000e+00, 3.4858e-01, 0.0000e+00],\n           [6.0009e-01, 7.1767e-01, 3.7594e-01, 3.0626e-01, 0.0000e+00],\n           [2.7731e-01, 0.0000e+00, 3.6853e-01, 8.4195e-01, 1.0147e+00],\n           [1.0103e+00, 0.0000e+00, 3.1503e-01, 0.0000e+00, 0.0000e+00]],\n\n          [[1.0323e-01, 0.0000e+00, 1.1268e-01, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 1.4197e+00, 0.0000e+00, 4.0874e-01],\n           [3.6228e-01, 0.0000e+00, 0.0000e+00, 6.2792e-01, 1.2368e+00],\n           [1.7522e-01, 9.1128e-01, 7.6131e-01, 0.0000e+00, 3.3104e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3391e-01]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [6.2109e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[4.9467e-01, 7.6377e-01, 0.0000e+00, 3.9802e-02, 0.0000e+00],\n           [4.5415e-01, 5.5685e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 6.0642e-01, 3.0420e-02, 2.9475e-01, 1.4615e-02],\n           [2.3578e-01, 4.9765e-01, 6.4014e-01, 1.6726e-01, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5307e-01, 6.0212e-01]],\n\n          [[2.8100e-01, 2.8905e-01, 0.0000e+00, 5.0438e-01, 6.6361e-02],\n           [3.1394e-01, 0.0000e+00, 7.6959e-01, 8.9534e-01, 3.9095e-01],\n           [1.4644e+00, 2.2778e+00, 2.2155e-01, 7.8384e-01, 6.1503e-01],\n           [4.0551e-01, 1.0914e+00, 2.1601e+00, 1.8593e+00, 0.0000e+00],\n           [1.3614e+00, 1.2524e+00, 1.3986e+00, 8.2003e-01, 1.8282e-01]]],\n\n\n         [[[9.9757e-01, 9.8742e-01, 2.0784e-01, 3.0223e-01, 0.0000e+00],\n           [4.3921e-01, 7.2234e-02, 1.5001e+00, 6.5133e-01, 0.0000e+00],\n           [8.0594e-03, 0.0000e+00, 0.0000e+00, 4.6346e-01, 0.0000e+00],\n           [3.2890e-01, 0.0000e+00, 5.5362e-01, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6695e-02]],\n\n          [[7.9303e-01, 1.3094e+00, 2.6984e-01, 5.4546e-01, 8.3356e-01],\n           [2.4585e-01, 0.0000e+00, 1.6852e+00, 0.0000e+00, 4.2651e-01],\n           [1.1114e-01, 8.0485e-01, 0.0000e+00, 0.0000e+00, 1.3826e+00],\n           [1.5069e+00, 8.1114e-01, 9.5529e-01, 0.0000e+00, 5.3504e-01],\n           [1.0211e+00, 3.7761e-01, 1.4626e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1658e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5961e-01],\n           [1.0144e-02, 2.4445e-01, 0.0000e+00, 0.0000e+00, 7.5595e-01],\n           [0.0000e+00, 8.9234e-01, 0.0000e+00, 0.0000e+00, 2.7408e-01],\n           [1.0491e+00, 1.8171e+00, 7.7375e-01, 1.8669e+00, 0.0000e+00]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5928e-01, 0.0000e+00],\n           [0.0000e+00, 6.5686e-01, 0.0000e+00, 9.0689e-01, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [4.8521e-01, 1.8774e+00, 4.7218e-01, 1.2614e+00, 0.0000e+00],\n           [3.0656e-02, 4.9621e-01, 0.0000e+00, 5.9379e-01, 0.0000e+00]],\n\n          [[6.1543e-01, 1.3678e+00, 6.2549e-01, 8.5195e-01, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 7.9455e-01, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.4867e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 7.2388e-01, 2.6937e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[1.7906e-01, 0.0000e+00, 6.9674e-01, 0.0000e+00, 1.5550e-01],\n           [4.3292e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.2240e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2489e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.2492e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n\n\n         [[[0.0000e+00, 5.9237e-01, 1.4700e-01, 0.0000e+00, 5.1108e-01],\n           [2.9226e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.9804e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0398e-01],\n           [0.0000e+00, 7.8306e-01, 0.0000e+00, 3.2075e-01, 1.2206e+00]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2461e-02],\n           [6.1029e-01, 2.4062e-01, 0.0000e+00, 0.0000e+00, 2.2722e-01],\n           [4.4496e-01, 1.2624e+00, 1.1538e-01, 0.0000e+00, 4.8510e-02],\n           [1.1850e+00, 1.7510e+00, 0.0000e+00, 0.0000e+00, 1.0512e+00],\n           [1.8706e+00, 2.6405e+00, 1.3558e+00, 1.5117e+00, 1.0069e+00]],\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0323e+00, 4.6487e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4703e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5300e-01]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [8.3295e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.2768e+00, 6.0554e-01, 9.1239e-01, 6.6587e-01, 7.4468e-01]],\n\n          [[1.1049e-01, 1.1937e+00, 1.1707e+00, 9.8171e-01, 9.0420e-01],\n           [9.3529e-01, 2.3034e+00, 1.8197e+00, 9.7649e-01, 1.3020e+00],\n           [4.0965e-01, 1.9252e+00, 2.3142e+00, 2.3196e+00, 2.3558e+00],\n           [1.8787e+00, 4.1617e+00, 3.5311e+00, 2.0912e+00, 2.7782e+00],\n           [4.4207e-01, 7.3675e-01, 2.5280e+00, 1.3643e+00, 1.9744e+00]],\n\n          [[0.0000e+00, 3.4759e-02, 0.0000e+00, 4.3093e-01, 2.6028e-01],\n           [1.3835e-01, 1.1607e+00, 8.8868e-01, 1.5226e+00, 5.9562e-01],\n           [3.9975e-01, 4.0906e-01, 7.9346e-01, 6.8147e-01, 0.0000e+00],\n           [0.0000e+00, 2.2586e+00, 0.0000e+00, 1.6276e+00, 1.1249e+00],\n           [1.1305e+00, 0.0000e+00, 7.4420e-01, 0.0000e+00, 0.0000e+00]]],\n\n\n         [[[2.7522e-01, 6.4387e-02, 2.3213e-01, 5.2220e-01, 2.6459e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7135e-02, 2.7806e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2122e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5661e-01],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[2.5686e-01, 8.8981e-01, 3.6860e-01, 5.5412e-01, 5.4487e-01],\n           [1.3535e+00, 8.4365e-01, 1.0677e+00, 1.8419e-01, 9.1419e-01],\n           [1.3212e+00, 0.0000e+00, 1.2446e+00, 5.3691e-01, 4.1759e-02],\n           [1.4828e+00, 2.9607e-01, 9.3667e-01, 9.4431e-01, 4.6710e-01],\n           [1.4142e+00, 1.0549e+00, 1.2679e+00, 1.5120e+00, 8.6681e-01]],\n\n          [[8.2361e-01, 1.4861e-01, 0.0000e+00, 7.2776e-02, 7.2024e-01],\n           [5.6011e-02, 3.1480e-01, 0.0000e+00, 0.0000e+00, 2.1081e-01],\n           [1.2067e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [4.7528e-01, 1.0114e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.7918e-01]],\n\n          ...,\n\n          [[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9671e-02, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n\n          [[8.0276e-01, 1.4493e+00, 8.6199e-01, 5.6391e-01, 2.3270e-01],\n           [1.0047e+00, 2.2127e+00, 1.5475e+00, 7.6110e-01, 7.5173e-01],\n           [1.3601e+00, 1.5159e+00, 1.8897e+00, 1.5189e+00, 8.7543e-01],\n           [6.7938e-01, 1.6053e+00, 1.3370e+00, 1.5455e+00, 1.0628e+00],\n           [5.2345e-01, 0.0000e+00, 1.6721e+00, 8.5783e-01, 1.0764e+00]],\n\n          [[2.7199e-01, 0.0000e+00, 0.0000e+00, 5.9157e-02, 0.0000e+00],\n           [5.5989e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n           [1.8424e+00, 0.0000e+00, 0.0000e+00, 2.8309e-01, 0.0000e+00],\n           [1.5794e+00, 4.9062e-02, 4.6284e-01, 2.3079e-01, 0.0000e+00],\n           [2.2750e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]]]],\n       device='cuda:0', grad_fn=<ViewBackward>)\n"
    }
   ],
   "source": [
    "print(sample_features_temp[0])\n",
    "print(sample_features_temp.reshape(5,5,64,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([75, 64, 5, 5])\n"
    }
   ],
   "source": [
    "batch_linear, batch_features = feature_encoder(batches)\n",
    "print(batch_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(25, 1600)\n(25,)\n(75, 1600)\n"
    }
   ],
   "source": [
    "sample_temp = np.array(sample_features.detach().cpu())\n",
    "sample_temp = sample_temp.reshape(25, -1)\n",
    "sample_temp_labels = np.array(sample_labels.detach().cpu())\n",
    "print(sample_temp.shape)\n",
    "print(sample_temp_labels.shape)\n",
    "\n",
    "clf.fit(sample_temp, sample_temp_labels)\n",
    "\n",
    "batch_temp = np.array(batch_features.detach().cpu())\n",
    "batch_temp = batch_temp.reshape(batch_temp.shape[0], -1)\n",
    "print(batch_temp.shape)\n",
    "batch_temp_label = np.array(batch_labels.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.22666666666666666"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "'''\n",
    "predict = clf.predict(test)\n",
    "true = np.array(batch_labels)\n",
    "assert predict.shape == true.shape\n",
    "\n",
    "accuracy_score(predict, true)\n",
    "'''\n",
    "\n",
    "predict = clf.predict(batch_temp)\n",
    "batch_temp_label = batch_temp_label.reshape(-1)\n",
    "assert predict.shape == batch_temp_label.shape, f\"{predict.shape}, {batch_temp_label.shape}\"\n",
    "\n",
    "accuracy_score(predict, batch_temp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.14166667 0.28333333 0.20833333 0.175      0.19166667]\n [0.20833333 0.2        0.19166667 0.25       0.15      ]\n [0.15833333 0.2        0.275      0.225      0.14166667]\n [0.18333333 0.175      0.2        0.18333333 0.25833333]\n [0.14166667 0.23333333 0.28333333 0.175      0.16666667]\n [0.19166667 0.34166667 0.13333333 0.20833333 0.125     ]\n [0.175      0.20833333 0.25833333 0.13333333 0.225     ]\n [0.11666667 0.3        0.19166667 0.26666667 0.125     ]\n [0.23333333 0.16666667 0.14166667 0.175      0.28333333]\n [0.225      0.26666667 0.21666667 0.15       0.14166667]\n [0.16666667 0.28333333 0.21666667 0.18333333 0.15      ]\n [0.10833333 0.25833333 0.25       0.175      0.20833333]\n [0.16666667 0.325      0.18333333 0.08333333 0.24166667]\n [0.175      0.225      0.26666667 0.225      0.10833333]\n [0.18333333 0.14166667 0.175      0.09166667 0.40833333]\n [0.10833333 0.40833333 0.1        0.25833333 0.125     ]\n [0.16666667 0.125      0.19166667 0.225      0.29166667]\n [0.175      0.21666667 0.175      0.25       0.18333333]\n [0.15       0.34166667 0.13333333 0.25       0.125     ]\n [0.29166667 0.1        0.14166667 0.21666667 0.25      ]\n [0.19166667 0.21666667 0.20833333 0.275      0.10833333]\n [0.19166667 0.20833333 0.14166667 0.175      0.28333333]\n [0.18333333 0.25833333 0.25       0.175      0.13333333]\n [0.25       0.175      0.275      0.175      0.125     ]\n [0.18333333 0.20833333 0.15833333 0.2        0.25      ]\n [0.175      0.20833333 0.16666667 0.175      0.275     ]\n [0.26666667 0.2        0.13333333 0.18333333 0.21666667]\n [0.20833333 0.20833333 0.16666667 0.18333333 0.23333333]\n [0.15833333 0.23333333 0.20833333 0.25833333 0.14166667]\n [0.11666667 0.25833333 0.125      0.30833333 0.19166667]\n [0.13333333 0.25833333 0.175      0.18333333 0.25      ]\n [0.225      0.18333333 0.2        0.21666667 0.175     ]\n [0.13333333 0.21666667 0.125      0.29166667 0.23333333]\n [0.16666667 0.20833333 0.15833333 0.20833333 0.25833333]\n [0.23333333 0.23333333 0.24166667 0.15833333 0.13333333]\n [0.26666667 0.26666667 0.15       0.1        0.21666667]\n [0.19166667 0.2        0.175      0.26666667 0.16666667]\n [0.09166667 0.36666667 0.21666667 0.175      0.15      ]\n [0.11666667 0.275      0.21666667 0.16666667 0.225     ]\n [0.175      0.21666667 0.29166667 0.2        0.11666667]\n [0.15       0.29166667 0.15833333 0.18333333 0.21666667]\n [0.15       0.15833333 0.24166667 0.25       0.2       ]\n [0.2        0.25       0.15833333 0.15833333 0.23333333]\n [0.20833333 0.225      0.2        0.25833333 0.10833333]\n [0.15833333 0.23333333 0.275      0.16666667 0.16666667]\n [0.18333333 0.26666667 0.15833333 0.21666667 0.175     ]\n [0.175      0.25       0.23333333 0.19166667 0.15      ]\n [0.225      0.25833333 0.125      0.175      0.21666667]\n [0.25       0.175      0.25833333 0.09166667 0.225     ]\n [0.175      0.28333333 0.25833333 0.15833333 0.125     ]\n [0.2        0.15       0.21666667 0.08333333 0.35      ]\n [0.18333333 0.2        0.175      0.26666667 0.175     ]\n [0.19166667 0.175      0.24166667 0.175      0.21666667]\n [0.26666667 0.24166667 0.18333333 0.2        0.10833333]\n [0.15       0.2        0.18333333 0.20833333 0.25833333]\n [0.11666667 0.31666667 0.16666667 0.275      0.125     ]\n [0.15       0.19166667 0.2        0.26666667 0.19166667]\n [0.175      0.21666667 0.21666667 0.25       0.14166667]\n [0.25       0.175      0.225      0.13333333 0.21666667]\n [0.225      0.19166667 0.225      0.18333333 0.175     ]\n [0.14166667 0.35       0.23333333 0.11666667 0.15833333]\n [0.075      0.45833333 0.11666667 0.20833333 0.14166667]\n [0.24166667 0.26666667 0.225      0.13333333 0.13333333]\n [0.15       0.24166667 0.19166667 0.15       0.26666667]\n [0.15       0.15833333 0.18333333 0.275      0.23333333]\n [0.14166667 0.36666667 0.2        0.14166667 0.15      ]\n [0.25       0.275      0.2        0.09166667 0.18333333]\n [0.225      0.275      0.15833333 0.125      0.21666667]\n [0.175      0.225      0.28333333 0.18333333 0.13333333]\n [0.21666667 0.20833333 0.225      0.15       0.2       ]\n [0.16666667 0.3        0.125      0.18333333 0.225     ]\n [0.15833333 0.25833333 0.13333333 0.275      0.175     ]\n [0.25       0.16666667 0.24166667 0.18333333 0.15833333]\n [0.2        0.19166667 0.18333333 0.26666667 0.15833333]\n [0.08333333 0.44166667 0.15       0.19166667 0.13333333]]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.37333333333333335"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators=120, random_state=1)\n",
    "clf2.fit(sample_temp, sample_temp_labels)\n",
    "\n",
    "predict2 = clf2.predict(batch_temp)\n",
    "batch_temp_label = batch_temp_label.reshape(-1)\n",
    "assert predict2.shape == batch_temp_label.shape, f\"{predict.shape}, {batch_temp_label.shape}\"\n",
    "\n",
    "print(clf2.predict_proba(batch_temp))\n",
    "accuracy_score(predict2, batch_temp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       0.23      0.47      0.31        15\n           1       0.53      0.53      0.53        15\n           2       0.23      0.20      0.21        15\n           3       0.22      0.13      0.17        15\n           4       0.12      0.07      0.09        15\n\n    accuracy                           0.28        75\n   macro avg       0.27      0.28      0.26        75\nweighted avg       0.27      0.28      0.26        75\n\n"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(batch_temp_label, predict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 1. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 1. 0.]\n [0. 1. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 1. 0.]\n [0. 0. 1. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 1. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 1. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]\n [0. 0. 1. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 1. 0. 0.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 1. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 1. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0.]]\n"
    }
   ],
   "source": [
    "print(clf.predict_proba(batch_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3,\n        3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, 3, 3, 3, 3,\n        3, 1, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n        2, 1, 3], device='cuda:0')\n0.14666666666666667\n"
    }
   ],
   "source": [
    "_, predicted = torch.max(batch_linear, 1)\n",
    "print(predicted)\n",
    "correct = (predicted == batch_labels).sum().item()\n",
    "print(correct/len(batch_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([5, 64, 5, 5])\n"
    }
   ],
   "source": [
    "'''\n",
    ">>> q = torch.tensor([[[1,2,3], [4,5,6]], [[7, 8, 9], [10, 11, 12]]])\n",
    ">>> q\n",
    "tensor([[[ 1,  2,  3],\n",
    "         [ 4,  5,  6]],\n",
    "\n",
    "        [[ 7,  8,  9],\n",
    "         [10, 11, 12]]])\n",
    ">>> q.shape\n",
    "torch.Size([2, 2, 3])\n",
    ">>> torch.sum(q, 1)\n",
    "tensor([[ 5,  7,  9],\n",
    "        [17, 19, 21]])\n",
    ">>> torch.sum(q, 1).squeeze(1)\n",
    "tensor([[ 5,  7,  9],\n",
    "        [17, 19, 21]])\n",
    "'''        \n",
    "sample_features = torch.sum(sample_features, 1)\n",
    "print(sample_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([5, 64, 5, 5])\n"
    }
   ],
   "source": [
    "sample_features = sample_features.squeeze(1)\n",
    "print(sample_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([75, 64, 5, 5])\n"
    }
   ],
   "source": [
    "batch_features, _ = feature_encoder(batches)\n",
    "print(batch_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 320 and input n_features is 1600 ",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-b816e4b96e27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_temp_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_temp_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \"\"\"\n\u001b[1;32m    418\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             raise ValueError(\"Number of features of the model must \"\n\u001b[0m\u001b[1;32m    389\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 320 and input n_features is 1600 "
     ]
    }
   ],
   "source": [
    "predict = clf.predict(batch_temp)\n",
    "\n",
    "assert predict.shape == batch_temp_label.shape\n",
    "\n",
    "accuracy_score(predict, batch_temp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([1, 5, 64, 5, 5])\ntorch.Size([75, 5, 64, 5, 5])\n"
    }
   ],
   "source": [
    "# * calculate relations\n",
    "# * each batch sample link to every samples to calculate relations\n",
    "# * to form a 100 * 128 matrix for relation network\n",
    "sample_features_ext = sample_features.unsqueeze(0)\n",
    "print(sample_features_ext.shape)\n",
    "\n",
    "sample_features_ext = sample_features_ext.repeat(BATCH_NUM_PER_CLASS * CLASS_NUM, 1, 1, 1, 1)\n",
    "print(sample_features_ext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([1, 75, 64, 5, 5])\ntorch.Size([5, 75, 64, 5, 5])\n"
    }
   ],
   "source": [
    "batch_features_ext = batch_features.unsqueeze(0)\n",
    "print(batch_features_ext.shape)\n",
    "\n",
    "batch_features_ext = batch_features_ext.repeat(CLASS_NUM, 1, 1, 1, 1)\n",
    "print(batch_features_ext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([75, 5, 64, 5, 5])\n"
    }
   ],
   "source": [
    "batch_features_ext = torch.transpose(batch_features_ext, 0, 1)\n",
    "print(batch_features_ext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([75, 5, 128, 5, 5])\ntorch.Size([375, 128, 5, 5])\n"
    }
   ],
   "source": [
    "relation_pairs = torch.cat((sample_features_ext, batch_features_ext), 2)\n",
    "print(relation_pairs.shape)\n",
    "\n",
    "relation_pairs = relation_pairs.view(-1, FEATURE_DIM * 2, 5, 5)\n",
    "print(relation_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([375, 1])\ntorch.Size([75, 5])\n"
    }
   ],
   "source": [
    "relations, features = relation_network(relation_pairs)\n",
    "print(relations.shape)\n",
    "\n",
    "relations = relations.view(-1, CLASS_NUM)\n",
    "print(relations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.7109, 0.7105, 0.7113, 0.7105, 0.7108],\n        [0.7107, 0.7104, 0.7111, 0.7105, 0.7108],\n        [0.7109, 0.7106, 0.7113, 0.7108, 0.7109],\n        [0.7111, 0.7107, 0.7116, 0.7109, 0.7107],\n        [0.7111, 0.7109, 0.7117, 0.7109, 0.7110],\n        [0.7110, 0.7108, 0.7114, 0.7107, 0.7107],\n        [0.7114, 0.7108, 0.7117, 0.7109, 0.7115],\n        [0.7113, 0.7110, 0.7115, 0.7109, 0.7112],\n        [0.7113, 0.7108, 0.7114, 0.7108, 0.7113],\n        [0.7111, 0.7107, 0.7118, 0.7107, 0.7110],\n        [0.7111, 0.7107, 0.7119, 0.7108, 0.7110],\n        [0.7109, 0.7108, 0.7114, 0.7106, 0.7108],\n        [0.7110, 0.7106, 0.7115, 0.7104, 0.7106],\n        [0.7111, 0.7107, 0.7114, 0.7108, 0.7108],\n        [0.7109, 0.7106, 0.7112, 0.7106, 0.7108],\n        [0.7109, 0.7108, 0.7116, 0.7106, 0.7109],\n        [0.7113, 0.7107, 0.7118, 0.7107, 0.7108],\n        [0.7115, 0.7109, 0.7113, 0.7109, 0.7107],\n        [0.7109, 0.7106, 0.7115, 0.7106, 0.7108],\n        [0.7112, 0.7110, 0.7117, 0.7110, 0.7109],\n        [0.7112, 0.7109, 0.7118, 0.7108, 0.7111],\n        [0.7109, 0.7109, 0.7114, 0.7107, 0.7108],\n        [0.7112, 0.7110, 0.7119, 0.7111, 0.7109],\n        [0.7113, 0.7110, 0.7116, 0.7108, 0.7110],\n        [0.7113, 0.7106, 0.7117, 0.7107, 0.7110],\n        [0.7112, 0.7107, 0.7116, 0.7106, 0.7109],\n        [0.7115, 0.7109, 0.7119, 0.7110, 0.7110],\n        [0.7115, 0.7113, 0.7117, 0.7113, 0.7115],\n        [0.7114, 0.7112, 0.7119, 0.7108, 0.7112],\n        [0.7108, 0.7106, 0.7115, 0.7108, 0.7109],\n        [0.7112, 0.7107, 0.7117, 0.7109, 0.7108],\n        [0.7114, 0.7110, 0.7117, 0.7111, 0.7111],\n        [0.7107, 0.7105, 0.7113, 0.7105, 0.7107],\n        [0.7109, 0.7109, 0.7116, 0.7106, 0.7108],\n        [0.7114, 0.7108, 0.7112, 0.7108, 0.7107],\n        [0.7110, 0.7108, 0.7117, 0.7108, 0.7105],\n        [0.7109, 0.7107, 0.7114, 0.7105, 0.7108],\n        [0.7111, 0.7108, 0.7114, 0.7106, 0.7109],\n        [0.7113, 0.7106, 0.7118, 0.7108, 0.7110],\n        [0.7110, 0.7107, 0.7118, 0.7107, 0.7109],\n        [0.7110, 0.7106, 0.7116, 0.7106, 0.7110],\n        [0.7112, 0.7108, 0.7119, 0.7107, 0.7111],\n        [0.7111, 0.7110, 0.7114, 0.7108, 0.7110],\n        [0.7107, 0.7107, 0.7112, 0.7105, 0.7106],\n        [0.7109, 0.7109, 0.7116, 0.7110, 0.7110],\n        [0.7115, 0.7108, 0.7118, 0.7108, 0.7110],\n        [0.7112, 0.7106, 0.7118, 0.7107, 0.7109],\n        [0.7113, 0.7109, 0.7120, 0.7107, 0.7110],\n        [0.7110, 0.7109, 0.7117, 0.7108, 0.7108],\n        [0.7113, 0.7107, 0.7117, 0.7106, 0.7109],\n        [0.7113, 0.7106, 0.7114, 0.7107, 0.7107],\n        [0.7108, 0.7108, 0.7115, 0.7110, 0.7109],\n        [0.7113, 0.7106, 0.7114, 0.7107, 0.7107],\n        [0.7110, 0.7107, 0.7114, 0.7106, 0.7108],\n        [0.7109, 0.7103, 0.7112, 0.7105, 0.7106],\n        [0.7111, 0.7110, 0.7113, 0.7109, 0.7108],\n        [0.7110, 0.7106, 0.7115, 0.7107, 0.7108],\n        [0.7112, 0.7106, 0.7117, 0.7107, 0.7108],\n        [0.7113, 0.7106, 0.7116, 0.7107, 0.7107],\n        [0.7112, 0.7110, 0.7115, 0.7108, 0.7109],\n        [0.7111, 0.7107, 0.7117, 0.7107, 0.7111],\n        [0.7110, 0.7108, 0.7112, 0.7106, 0.7109],\n        [0.7111, 0.7109, 0.7114, 0.7106, 0.7109],\n        [0.7113, 0.7108, 0.7117, 0.7109, 0.7109],\n        [0.7112, 0.7110, 0.7114, 0.7108, 0.7110],\n        [0.7109, 0.7107, 0.7114, 0.7106, 0.7108],\n        [0.7111, 0.7107, 0.7113, 0.7106, 0.7109],\n        [0.7107, 0.7105, 0.7112, 0.7107, 0.7111],\n        [0.7111, 0.7108, 0.7114, 0.7107, 0.7107],\n        [0.7113, 0.7109, 0.7115, 0.7107, 0.7110],\n        [0.7109, 0.7106, 0.7115, 0.7108, 0.7111],\n        [0.7109, 0.7108, 0.7115, 0.7106, 0.7109],\n        [0.7110, 0.7109, 0.7113, 0.7107, 0.7109],\n        [0.7110, 0.7105, 0.7113, 0.7105, 0.7104],\n        [0.7109, 0.7107, 0.7112, 0.7104, 0.7110]], device='cuda:0',\n       grad_fn=<ViewBackward>)\ntensor([[0.2000, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.2000, 0.1999],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2002, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2001, 0.1999, 0.2001, 0.1999, 0.1999],\n        [0.2001, 0.2000, 0.2001, 0.2000, 0.1999],\n        [0.2000, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.1999],\n        [0.2000, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.1999],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.2000, 0.1999],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.2000, 0.1999],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2001, 0.2000, 0.2000, 0.2000, 0.1999],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.1999],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2002, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2002, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.2000],\n        [0.2001, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2002, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2002, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2001, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2001, 0.1999, 0.2001, 0.2000, 0.1999],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.2000],\n        [0.2001, 0.1999, 0.2001, 0.1999, 0.1999],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2001, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.2000, 0.1999],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.1999, 0.2001, 0.2000, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000],\n        [0.2000, 0.2000, 0.2001, 0.2000, 0.2000],\n        [0.2001, 0.2000, 0.2001, 0.1999, 0.1999],\n        [0.2000, 0.2000, 0.2001, 0.1999, 0.2000]], device='cuda:0',\n       grad_fn=<SoftmaxBackward>)\n"
    }
   ],
   "source": [
    "print(relations)\n",
    "print(torch.softmax(relations, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([375, 64, 1, 1])\n"
    }
   ],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([75, 5])\ntorch.Size([75, 5])\n"
    }
   ],
   "source": [
    "mse = nn.MSELoss()\n",
    "one_hot_labels = torch.zeros(BATCH_NUM_PER_CLASS * CLASS_NUM, CLASS_NUM).to(device).scatter_(1, batch_labels.view(-1, 1), 1)\n",
    "print(one_hot_labels.shape)\n",
    "print(relations.shape)\n",
    "loss = mse(relations, one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([75])\n"
    }
   ],
   "source": [
    "print(batch_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.42929226\n"
    }
   ],
   "source": [
    "print(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_encoder.zero_grad()\n",
    "relation_network.zero_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "torch.nn.utils.clip_grad_norm(feature_encoder.parameters(), 0.5)\n",
    "torch.nn.utils.clip_grad_norm(relation_network.parameters(), 0.5)\n",
    "\n",
    "feature_encoder_optim.step()\n",
    "relation_network_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.42929226\n"
    }
   ],
   "source": [
    "print(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "episode : 10, loss : 0.4160117506980896  \n",
    "episode : 20, loss : 0.363679438829422  \n",
    "episode : 30, loss : 0.28375476598739624  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test all iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training...\nepisode : 100, loss : 0.3154416084289551\nepisode : 200, loss : 0.2484121322631836\nepisode : 300, loss : 0.2421010285615921\nepisode : 400, loss : 0.2561458647251129\nepisode : 500, loss : 0.23913037776947021\nepisode : 600, loss : 0.2268974781036377\nepisode : 700, loss : 0.20814134180545807\nepisode : 800, loss : 0.20158953964710236\nepisode : 900, loss : 0.2230621874332428\nepisode : 1000, loss : 0.20491725206375122\nepisode : 1100, loss : 0.22936466336250305\nepisode : 1200, loss : 0.22024692595005035\nepisode : 1300, loss : 0.20556291937828064\nepisode : 1400, loss : 0.1771606206893921\nepisode : 1500, loss : 0.17625921964645386\nepisode : 1600, loss : 0.21023140847682953\nepisode : 1700, loss : 0.20093770325183868\nepisode : 1800, loss : 0.2110174596309662\nepisode : 1900, loss : 0.24598142504692078\nepisode : 2000, loss : 0.20057158172130585\nepisode : 2100, loss : 0.20276468992233276\nepisode : 2200, loss : 0.19725066423416138\nepisode : 2300, loss : 0.20562830567359924\nepisode : 2400, loss : 0.18037159740924835\nepisode : 2500, loss : 0.20806202292442322\nTesting...\ntest accuracy :  0.93264\nsave networks for episode: 2499\nepisode : 2600, loss : 0.18393076956272125\nepisode : 2700, loss : 0.1938244104385376\nepisode : 2800, loss : 0.2036685049533844\nepisode : 2900, loss : 0.19208106398582458\nepisode : 3000, loss : 0.22736452519893646\nepisode : 3100, loss : 0.19703921675682068\nepisode : 3200, loss : 0.17870372533798218\nepisode : 3300, loss : 0.18477874994277954\nepisode : 3400, loss : 0.18826794624328613\nepisode : 3500, loss : 0.19254766404628754\nepisode : 3600, loss : 0.1996447592973709\nepisode : 3700, loss : 0.2147994339466095\nepisode : 3800, loss : 0.2066645622253418\nepisode : 3900, loss : 0.20548653602600098\nepisode : 4000, loss : 0.19541242718696594\nepisode : 4100, loss : 0.18640610575675964\nepisode : 4200, loss : 0.18212515115737915\nepisode : 4300, loss : 0.1792658418416977\nepisode : 4400, loss : 0.1917852759361267\nepisode : 4500, loss : 0.16907986998558044\nepisode : 4600, loss : 0.22969792783260345\nepisode : 4700, loss : 0.1870693564414978\nepisode : 4800, loss : 0.1808556318283081\nepisode : 4900, loss : 0.20307287573814392\nepisode : 5000, loss : 0.20744934678077698\nTesting...\ntest accuracy :  0.9596399999999999\nsave networks for episode: 4999\nepisode : 5100, loss : 0.18044395744800568\nepisode : 5200, loss : 0.206318661570549\nepisode : 5300, loss : 0.20393145084381104\nepisode : 5400, loss : 0.16739754378795624\nepisode : 5500, loss : 0.1924794316291809\nepisode : 5600, loss : 0.17886552214622498\nepisode : 5700, loss : 0.1841164529323578\nepisode : 5800, loss : 0.18089976906776428\nepisode : 5900, loss : 0.18821348249912262\nepisode : 6000, loss : 0.19133922457695007\nepisode : 6100, loss : 0.17233198881149292\nepisode : 6200, loss : 0.1994836926460266\nepisode : 6300, loss : 0.1833954006433487\nepisode : 6400, loss : 0.194179967045784\nepisode : 6500, loss : 0.1861884593963623\nepisode : 6600, loss : 0.1681845635175705\nepisode : 6700, loss : 0.17667213082313538\nepisode : 6800, loss : 0.22091560065746307\nepisode : 6900, loss : 0.2178378403186798\nepisode : 7000, loss : 0.19184857606887817\nepisode : 7100, loss : 0.1947050392627716\nepisode : 7200, loss : 0.1790599673986435\nepisode : 7300, loss : 0.16876044869422913\nepisode : 7400, loss : 0.19196484982967377\nepisode : 7500, loss : 0.18492509424686432\nTesting...\ntest accuracy :  0.96672\nsave networks for episode: 7499\nepisode : 7600, loss : 0.17301811277866364\nepisode : 7700, loss : 0.17905834317207336\nepisode : 7800, loss : 0.19316674768924713\nepisode : 7900, loss : 0.17931750416755676\nepisode : 8000, loss : 0.18277296423912048\nepisode : 8100, loss : 0.24021229147911072\nepisode : 8200, loss : 0.1997741460800171\nepisode : 8300, loss : 0.2002992033958435\nepisode : 8400, loss : 0.17934396862983704\nepisode : 8500, loss : 0.16938123106956482\nepisode : 8600, loss : 0.18397372961044312\nepisode : 8700, loss : 0.16718138754367828\nepisode : 8800, loss : 0.1832645833492279\nepisode : 8900, loss : 0.17129622399806976\nepisode : 9000, loss : 0.1696842461824417\nepisode : 9100, loss : 0.22265739738941193\nepisode : 9200, loss : 0.17290261387825012\nepisode : 9300, loss : 0.1882312297821045\nepisode : 9400, loss : 0.16694839298725128\nepisode : 9500, loss : 0.16611726582050323\nepisode : 9600, loss : 0.18211697041988373\nepisode : 9700, loss : 0.2097700536251068\nepisode : 9800, loss : 0.18730080127716064\nepisode : 9900, loss : 0.16955828666687012\nepisode : 10000, loss : 0.2021569311618805\nTesting...\ntest accuracy :  0.9731200000000001\nsave networks for episode: 9999\nepisode : 10100, loss : 0.1922217458486557\nepisode : 10200, loss : 0.18475499749183655\nepisode : 10300, loss : 0.1806766539812088\nepisode : 10400, loss : 0.17843309044837952\nepisode : 10500, loss : 0.1870095133781433\nepisode : 10600, loss : 0.1752028614282608\nepisode : 10700, loss : 0.1690702736377716\nepisode : 10800, loss : 0.1652398407459259\nepisode : 10900, loss : 0.19026540219783783\nepisode : 11000, loss : 0.177115336060524\nepisode : 11100, loss : 0.170701801776886\nepisode : 11200, loss : 0.17276664078235626\nepisode : 11300, loss : 0.17833080887794495\nepisode : 11400, loss : 0.22655239701271057\nepisode : 11500, loss : 0.1980402171611786\nepisode : 11600, loss : 0.16989126801490784\nepisode : 11700, loss : 0.1665487289428711\nepisode : 11800, loss : 0.2009173184633255\nepisode : 11900, loss : 0.18091042339801788\nepisode : 12000, loss : 0.18466301262378693\nepisode : 12100, loss : 0.2053355723619461\nepisode : 12200, loss : 0.19260460138320923\nepisode : 12300, loss : 0.19090168178081512\nepisode : 12400, loss : 0.16686169803142548\nepisode : 12500, loss : 0.1839299201965332\nTesting...\ntest accuracy :  0.97372\nsave networks for episode: 12499\nepisode : 12600, loss : 0.17899614572525024\nepisode : 12700, loss : 0.17297838628292084\nepisode : 12800, loss : 0.17475110292434692\nepisode : 12900, loss : 0.18356508016586304\nepisode : 13000, loss : 0.1751408576965332\nepisode : 13100, loss : 0.18161886930465698\nepisode : 13200, loss : 0.1822323501110077\nepisode : 13300, loss : 0.18206992745399475\nepisode : 13400, loss : 0.17254666984081268\nepisode : 13500, loss : 0.23784567415714264\nepisode : 13600, loss : 0.17294473946094513\nepisode : 13700, loss : 0.16563376784324646\nepisode : 13800, loss : 0.17393167316913605\nepisode : 13900, loss : 0.18169261515140533\nepisode : 14000, loss : 0.1748596727848053\nepisode : 14100, loss : 0.17384813725948334\nepisode : 14200, loss : 0.16595424711704254\nepisode : 14300, loss : 0.17906199395656586\nepisode : 14400, loss : 0.17796026170253754\nepisode : 14500, loss : 0.18976423144340515\nepisode : 14600, loss : 0.17376156151294708\nepisode : 14700, loss : 0.18318520486354828\nepisode : 14800, loss : 0.16521066427230835\nepisode : 14900, loss : 0.18946035206317902\nepisode : 15000, loss : 0.18243622779846191\nTesting...\ntest accuracy :  0.9762799999999999\nsave networks for episode: 14999\nepisode : 15100, loss : 0.19057923555374146\nepisode : 15200, loss : 0.175677090883255\nepisode : 15300, loss : 0.1697634905576706\nepisode : 15400, loss : 0.16338825225830078\nepisode : 15500, loss : 0.1734379082918167\nepisode : 15600, loss : 0.19308143854141235\nepisode : 15700, loss : 0.1630331575870514\nepisode : 15800, loss : 0.205271914601326\nepisode : 15900, loss : 0.16931334137916565\nepisode : 16000, loss : 0.17754624783992767\nepisode : 16100, loss : 0.18108761310577393\nepisode : 16200, loss : 0.17842601239681244\nepisode : 16300, loss : 0.1765916347503662\nepisode : 16400, loss : 0.1742025464773178\nepisode : 16500, loss : 0.17021828889846802\nepisode : 16600, loss : 0.18790562450885773\nepisode : 16700, loss : 0.16167737543582916\nepisode : 16800, loss : 0.18275387585163116\nepisode : 16900, loss : 0.2050129771232605\nepisode : 17000, loss : 0.1786440759897232\nepisode : 17100, loss : 0.2019168883562088\nepisode : 17200, loss : 0.18041273951530457\nepisode : 17300, loss : 0.16291634738445282\nepisode : 17400, loss : 0.16831275820732117\nepisode : 17500, loss : 0.17456413805484772\nTesting...\ntest accuracy :  0.9791200000000001\nsave networks for episode: 17499\nepisode : 17600, loss : 0.20461928844451904\nepisode : 17700, loss : 0.16289880871772766\nepisode : 17800, loss : 0.17923104763031006\nepisode : 17900, loss : 0.17182603478431702\nepisode : 18000, loss : 0.1815832555294037\nepisode : 18100, loss : 0.1609153300523758\nepisode : 18200, loss : 0.17711055278778076\nepisode : 18300, loss : 0.21652176976203918\nepisode : 18400, loss : 0.17467892169952393\nepisode : 18500, loss : 0.1697535663843155\nepisode : 18600, loss : 0.18206822872161865\nepisode : 18700, loss : 0.1697036623954773\nepisode : 18800, loss : 0.18764276802539825\nepisode : 18900, loss : 0.19438377022743225\nepisode : 19000, loss : 0.16761593520641327\nepisode : 19100, loss : 0.18248990178108215\nepisode : 19200, loss : 0.16841335594654083\nepisode : 19300, loss : 0.18186524510383606\nepisode : 19400, loss : 0.19398069381713867\nepisode : 19500, loss : 0.1737363040447235\nepisode : 19600, loss : 0.16857996582984924\nepisode : 19700, loss : 0.1676284223794937\nepisode : 19800, loss : 0.1757780760526657\nepisode : 19900, loss : 0.16975127160549164\nepisode : 20000, loss : 0.16847611963748932\nTesting...\ntest accuracy :  0.9818\nsave networks for episode: 19999\nepisode : 20100, loss : 0.17180094122886658\nepisode : 20200, loss : 0.17551743984222412\nepisode : 20300, loss : 0.16612522304058075\nepisode : 20400, loss : 0.2012922167778015\nepisode : 20500, loss : 0.17900608479976654\nepisode : 20600, loss : 0.17059378325939178\nepisode : 20700, loss : 0.17860367894172668\nepisode : 20800, loss : 0.17527984082698822\nepisode : 20900, loss : 0.1791965514421463\nepisode : 21000, loss : 0.17532123625278473\nepisode : 21100, loss : 0.17140045762062073\nepisode : 21200, loss : 0.1713496297597885\nepisode : 21300, loss : 0.1924646496772766\nepisode : 21400, loss : 0.17004737257957458\n"
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "\n",
    "last_accuracy = 0.0\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "for episode in range(EPISODE):\n",
    "    \n",
    "    #feature_encoder_scheduler.step(episode)\n",
    "    #relation_network_scheduler.step(episode)\n",
    "    \n",
    "    # * init dataset\n",
    "    # * sample_dataloader is to obtain previous samples for compare\n",
    "    # * batch_dataloader is to batch samples for training\n",
    "    degrees = random.choice([0, 90, 180, 270])\n",
    "    task = tg.OmniglotTask(metatrain_character_folders, CLASS_NUM, SAMPLE_NUM_PER_CLASS, BATCH_NUM_PER_CLASS)\n",
    "    sample_dataloader = tg.get_data_loader(task, num_per_class=SAMPLE_NUM_PER_CLASS, split=\"train\", shuffle=False, rotation=degrees)\n",
    "    batch_dataloader = tg.get_data_loader(task, num_per_class=BATCH_NUM_PER_CLASS, split=\"test\", shuffle=True, rotation=degrees)\n",
    "    \n",
    "    # * sample datas\n",
    "    # samples, sample_labels = sample_dataloader.__iter__().next()\n",
    "    # batches, batch_labels = batch_dataloader.__iter__().next()\n",
    "    \n",
    "    samples, sample_labels = next(iter(sample_dataloader))\n",
    "    batches, batch_labels = next(iter(batch_dataloader))\n",
    "    \n",
    "    samples, sample_labels = samples.to(device), sample_labels.to(device)\n",
    "    batches, batch_labels = batches.to(device), batch_labels.to(device)\n",
    "        \n",
    "    one_hot_sample_labels = torch.zeros(SAMPLE_NUM_PER_CLASS * CLASS_NUM, CLASS_NUM).to(device).scatter_(1, sample_labels.view(-1, 1), 1)\n",
    "\n",
    "    # * calculates features\n",
    "    sample_features, linear = feature_encoder(samples)\n",
    "    sample_features = sample_features.view(CLASS_NUM, SAMPLE_NUM_PER_CLASS, FEATURE_DIM, 5, 5)\n",
    "    sample_features = torch.sum(sample_features, 1).squeeze(1)\n",
    "    batch_features, _ = feature_encoder(batches)\n",
    "    \n",
    "    embedding_loss = mse(linear, one_hot_sample_labels)\n",
    "\n",
    "    # * calculate relations\n",
    "    # * each batch sample link to every samples to calculate relations\n",
    "    # * to form a 100 * 128 matrix for relation network\n",
    "    sample_features_ext = sample_features.unsqueeze(0).repeat(BATCH_NUM_PER_CLASS * CLASS_NUM, 1, 1, 1, 1)\n",
    "    batch_features_ext = batch_features.unsqueeze(0).repeat(CLASS_NUM, 1, 1, 1, 1)\n",
    "    batch_features_ext = torch.transpose(batch_features_ext, 0, 1)\n",
    "    \n",
    "    relation_pairs = torch.cat((sample_features_ext, batch_features_ext), 2).view(-1, FEATURE_DIM * 2, 5, 5)\n",
    "    relations = relation_network(relation_pairs).view(-1, CLASS_NUM)\n",
    "    \n",
    "\n",
    "    one_hot_labels = torch.zeros(BATCH_NUM_PER_CLASS * CLASS_NUM, CLASS_NUM).to(device).scatter_(1, batch_labels.view(-1, 1), 1)\n",
    "    loss = mse(relations, one_hot_labels)\n",
    "    loss += embedding_loss\n",
    "\n",
    "    feature_encoder.zero_grad()\n",
    "    relation_network.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm(feature_encoder.parameters(), 0.5)\n",
    "    torch.nn.utils.clip_grad_norm(relation_network.parameters(), 0.5)\n",
    "    \n",
    "    feature_encoder_optim.step()\n",
    "    relation_network_optim.step()\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"episode : {episode+1}, loss : {loss.cpu().detach().numpy()}\")\n",
    "        \n",
    "    if (episode + 1) % 2500 == 0:\n",
    "        print(\"Testing...\")\n",
    "        total_reward = 0\n",
    "        \n",
    "        for i in range(TEST_EPISODE):\n",
    "            degrees = random.choice([0, 90, 180, 270])\n",
    "            task = tg.OmniglotTask(metatest_character_folders, CLASS_NUM, SAMPLE_NUM_PER_CLASS, SAMPLE_NUM_PER_CLASS)\n",
    "            sample_dataloader = tg.get_data_loader(task, num_per_class=SAMPLE_NUM_PER_CLASS, split=\"train\", shuffle=False, rotation=degrees)\n",
    "            test_dataloader = tg.get_data_loader(task, num_per_class=SAMPLE_NUM_PER_CLASS, split=\"test\", shuffle=True, rotation=degrees)\n",
    "\n",
    "            # sample_images, sample_labels = sample_dataloader.__iter__().next()\n",
    "            # test_images, test_labels = test_dataloader.__iter__().next()            \n",
    "\n",
    "            sample_images, sample_labels = next(iter(sample_dataloader))\n",
    "            test_images, test_labels = next(iter(test_dataloader))\n",
    "\n",
    "            sample_images, sample_labels = sample_images.to(device), sample_labels.to(device)\n",
    "            test_images, test_labels = test_images.to(device), test_labels.to(device)\n",
    "                \n",
    "            # * calculate features\n",
    "            sample_features, _ = feature_encoder(sample_images)\n",
    "            sample_features = sample_features.view(CLASS_NUM, SAMPLE_NUM_PER_CLASS, FEATURE_DIM, 5, 5)\n",
    "            sample_features = torch.sum(sample_features, 1).squeeze(1)\n",
    "            test_features, _ = feature_encoder(test_images)\n",
    "            \n",
    "            # * calculate relations\n",
    "            # * each batch sample link to every samples to calculate relations\n",
    "            # * to form a 100x128 matrix for relation network\n",
    "            \n",
    "            sample_features_ext = sample_features.unsqueeze(0).repeat(SAMPLE_NUM_PER_CLASS * CLASS_NUM, 1, 1, 1, 1)\n",
    "            test_features_ext = test_features.unsqueeze(0).repeat(CLASS_NUM, 1, 1, 1, 1)\n",
    "            test_features_ext = torch.transpose(test_features_ext, 0, 1)         \n",
    "\n",
    "            relation_pairs = torch.cat((sample_features_ext, test_features_ext), 2).view(-1, FEATURE_DIM * 2, 5, 5)\n",
    "            relations = relation_network(relation_pairs).view(-1, CLASS_NUM)\n",
    "            \n",
    "            _, predict_labels = torch.max(relations.data, 1)\n",
    "            \n",
    "            rewards = [1 if predict_labels[j] == test_labels[j] else 0 for j in range(CLASS_NUM * SAMPLE_NUM_PER_CLASS)]\n",
    "            total_reward += np.sum(rewards)\n",
    "            \n",
    "        test_accuracy = total_reward/1.0/CLASS_NUM/SAMPLE_NUM_PER_CLASS/TEST_EPISODE\n",
    "        # total_reward / (1.0 * CLASS_NUM * SAMPLE_NUM_PER_CLASS * TEST_EPISODE)\n",
    "        \n",
    "        print(\"test accuracy : \", test_accuracy)\n",
    "        \n",
    "        if test_accuracy > last_accuracy:\n",
    "            # save networks\n",
    "            torch.save(\n",
    "                feature_encoder.state_dict(),\n",
    "                str(\"./models/omniglot_feature_encoder_\" + str(CLASS_NUM) + \"way_\" + str(SAMPLE_NUM_PER_CLASS) + \"shot.pkl\")\n",
    "            )\n",
    "\n",
    "            torch.save(\n",
    "                relation_network.state_dict(),\n",
    "                str(\"./models/omniglot_relation_network_\" + str(CLASS_NUM) + \"way_\" + str(SAMPLE_NUM_PER_CLASS) + \"shot.pkl\")\n",
    "            )\n",
    "\n",
    "            print(\"save networks for episode:\", episode)\n",
    "            last_accuracy = test_accuracy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38164bit66f7a8720ea44564891eb6b9b39e6c03",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}