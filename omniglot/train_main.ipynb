{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omniglot_train_few_shot_no_args as ot\n",
    "import task_generator_no_args as tg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "python : 3.8.2 (default, Apr  8 2020, 14:31:25) \n[GCC 9.3.0]\npytorch : 1.4.0\n"
    }
   ],
   "source": [
    "import sys\n",
    "print(f'python : {sys.version}')\n",
    "print(f'pytorch : {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_DIM = 64  \n",
    "RELATION_DIM = 8  \n",
    "CLASS_NUM = 5  \n",
    "SAMPLE_NUM_PER_CLASS = 5  \n",
    "BATCH_NUM_PER_CLASS = 15  \n",
    "EPISODE = 1000000  \n",
    "TEST_EPISODE = 1000  \n",
    "LEARNING_RATE = 0.001  \n",
    "HIDDEN_UNIT = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test just 1 iteration to know how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "init data folders\n"
    }
   ],
   "source": [
    "# * Step 1: init data folders\n",
    "print(\"init data folders\")\n",
    "\n",
    "# * Init character folders for dataset construction\n",
    "metatrain_character_folders, metatest_character_folders = tg.omniglot_character_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "init neural networks\n"
    }
   ],
   "source": [
    "# * Step 2: init neural networks\n",
    "print(\"init neural networks\")\n",
    "\n",
    "feature_encoder = ot.CNNEncoder()\n",
    "relation_network = ot.RelationNetwork(FEATURE_DIM, RELATION_DIM)\n",
    "\n",
    "feature_encoder.apply(ot.weights_init)\n",
    "relation_network.apply(ot.weights_init)\n",
    "\n",
    "feature_encoder.to(device)\n",
    "relation_network.to(device)\n",
    "    \n",
    "feature_encoder_optim = torch.optim.Adam(feature_encoder.parameters(), lr=LEARNING_RATE)\n",
    "#feature_encoder_scheduler = ot.StepLR(feature_encoder_optim, step_size=100000, gamma=0.5)\n",
    "\n",
    "relation_network_optim = torch.optim.Adam(relation_network.parameters(), lr=LEARNING_RATE)\n",
    "#relation_network_scheduler = ot.StepLR(relation_network_optim, step_size=100000, gamma=0.5)\n",
    "\n",
    "if os.path.exists(str(\"./models/omniglot_feature_encoder_\" + str(CLASS_NUM) + \"way_\" + str(SAMPLE_NUM_PER_CLASS) + \"shot.pkl\")):\n",
    "    feature_encoder.load_state_dict(torch.load(str(\"./models/omniglot_feature_encoder_\" + str(CLASS_NUM) + \"way_\" + str(SAMPLE_NUM_PER_CLASS) + \"shot.pkl\")))\n",
    "    print(\"load feature encoder success\")\n",
    "if os.path.exists(str(\"./models/omniglot_relation_network_\" + str(CLASS_NUM) + \"way_\" + str(SAMPLE_NUM_PER_CLASS) + \"shot.pkl\")):\n",
    "    relation_network.load_state_dict(torch.load(str(\"./models/omniglot_relation_network_\" + str(CLASS_NUM) + \"way_\" + str(SAMPLE_NUM_PER_CLASS) + \"shot.pkl\")))\n",
    "    print(\"load relation network success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 1\n",
    "#feature_encoder_scheduler.step(episode)\n",
    "#relation_network_scheduler.step(episode)\n",
    "\n",
    "# * init dataset\n",
    "# * sample_dataloader is to obtain previous samples for compare\n",
    "# * batch_dataloader is to batch samples for training\n",
    "degrees = random.choice([0, 90, 180, 270])\n",
    "task = tg.OmniglotTask(metatrain_character_folders, CLASS_NUM, SAMPLE_NUM_PER_CLASS, BATCH_NUM_PER_CLASS)\n",
    "sample_dataloader = tg.get_data_loader(task, num_per_class=SAMPLE_NUM_PER_CLASS, split=\"train\", shuffle=False, rotation=degrees)\n",
    "batch_dataloader = tg.get_data_loader(task, num_per_class=BATCH_NUM_PER_CLASS, split=\"test\", shuffle=True, rotation=degrees)\n",
    "\n",
    "# * sample datas\n",
    "samples, sample_labels = next(iter(sample_dataloader))  # sample_dataloader.__iter__().next()\n",
    "batches, batch_labels = next(iter(batch_dataloader))  # batch_dataloader.__iter__().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(0)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7ff098bfe370>"
     },
     "metadata": {},
     "execution_count": 6
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 251.565 248.518125 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \nL 244.365 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p3187e1d5a8)\">\n    <image height=\"218\" id=\"image3e93dde9e5\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAABHNCSVQICAgIfAhkiAAAAvBJREFUeJzt3VFqwzAUAMG69P5XVn/zUxsatIqtmRMoCcuDPGQfY4zxBUz1vfoAsAOhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBoGf1Qe4q+M4Vh9hijHG6iM8kokGAaFBQGgQEBoEhAYBoUFAaBCwR/vDu3uyT95HnX22J3/ulUw0CAgNAkKDgNAgIDQICA0CQoPAtnu0nfdFdz77XZloEBAaBIQGAaFBQGgQEBoEhAaBbfdoV7ukpz63kTVMNAgIDQJCg4DQICA0CAgNAtv+vf+uq7//XUXhlYkGAaFBQGgQEBoEhAYBoUFAaBA4hoXPv7xzjcZXvh8TDQJCg4DQICA0CAgNAkKDgNAgYI82yczH1fnJ7sdEg4DQICA0CAgNAkKDgNAgIDQIeK7jJGe7Lq+E2o+JBgGhQUBoEBAaBIQGAaFBQGgQsEeb5GxX5j7Zfkw0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAq7JLHD1uLmrazReCXU/JhoEhAYBoUFAaBAQGgSEBgGhQcAe7QN5rdPzmGgQEBoEhAYBoUFAaBAQGgSEBgF7tEnc6+KViQYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAaBX0cdN7qFTOuFAAAAAElFTkSuQmCC\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m031cabc4d7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m031cabc4d7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m031cabc4d7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m031cabc4d7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m031cabc4d7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m031cabc4d7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m031cabc4d7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m4246df6bbf\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4246df6bbf\" y=\"11.082857\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4246df6bbf\" y=\"49.911429\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4246df6bbf\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4246df6bbf\" y=\"127.568571\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4246df6bbf\" y=\"166.397143\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4246df6bbf\" y=\"205.225714\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 224.64 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p3187e1d5a8\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "print(sample_labels[0])\n",
    "plt.imshow(samples[0].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, sample_labels = samples.to(device), sample_labels.to(device)\n",
    "batches, batch_labels = batches.to(device), batch_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([25, 64, 5, 5])\n"
    }
   ],
   "source": [
    "# * calculates features\n",
    "sample_features = feature_encoder(samples)\n",
    "print(sample_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([5, 5, 64, 5, 5])\n"
    }
   ],
   "source": [
    "sample_features = sample_features.view(CLASS_NUM, SAMPLE_NUM_PER_CLASS, FEATURE_DIM, 5, 5)\n",
    "print(sample_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([5, 64, 5, 5])\n"
    }
   ],
   "source": [
    "'''\n",
    ">>> q = torch.tensor([[[1,2,3], [4,5,6]], [[7, 8, 9], [10, 11, 12]]])\n",
    ">>> q\n",
    "tensor([[[ 1,  2,  3],\n",
    "         [ 4,  5,  6]],\n",
    "\n",
    "        [[ 7,  8,  9],\n",
    "         [10, 11, 12]]])\n",
    ">>> q.shape\n",
    "torch.Size([2, 2, 3])\n",
    ">>> torch.sum(q, 1)\n",
    "tensor([[ 5,  7,  9],\n",
    "        [17, 19, 21]])\n",
    ">>> torch.sum(q, 1).squeeze(1)\n",
    "tensor([[ 5,  7,  9],\n",
    "        [17, 19, 21]])\n",
    "'''        \n",
    "sample_features = torch.sum(sample_features, 1)\n",
    "print(sample_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([5, 64, 5, 5])\n"
    }
   ],
   "source": [
    "sample_features = sample_features.squeeze(1)\n",
    "print(sample_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([75, 64, 5, 5])\n"
    }
   ],
   "source": [
    "batch_features = feature_encoder(batches)\n",
    "print(batch_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([1, 5, 64, 5, 5])\ntorch.Size([75, 5, 64, 5, 5])\n"
    }
   ],
   "source": [
    "# * calculate relations\n",
    "# * each batch sample link to every samples to calculate relations\n",
    "# * to form a 100 * 128 matrix for relation network\n",
    "sample_features_ext = sample_features.unsqueeze(0)\n",
    "print(sample_features_ext.shape)\n",
    "\n",
    "sample_features_ext = sample_features_ext.repeat(BATCH_NUM_PER_CLASS * CLASS_NUM, 1, 1, 1, 1)\n",
    "print(sample_features_ext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([1, 75, 64, 5, 5])\ntorch.Size([5, 75, 64, 5, 5])\n"
    }
   ],
   "source": [
    "batch_features_ext = batch_features.unsqueeze(0)\n",
    "print(batch_features_ext.shape)\n",
    "\n",
    "batch_features_ext = batch_features_ext.repeat(CLASS_NUM, 1, 1, 1, 1)\n",
    "print(batch_features_ext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([75, 5, 64, 5, 5])\n"
    }
   ],
   "source": [
    "batch_features_ext = torch.transpose(batch_features_ext, 0, 1)\n",
    "print(batch_features_ext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([75, 5, 128, 5, 5])\ntorch.Size([375, 128, 5, 5])\n"
    }
   ],
   "source": [
    "relation_pairs = torch.cat((sample_features_ext, batch_features_ext), 2)\n",
    "print(relation_pairs.shape)\n",
    "\n",
    "relation_pairs = relation_pairs.view(-1, FEATURE_DIM * 2, 5, 5)\n",
    "print(relation_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([375, 1])\ntorch.Size([75, 5])\n"
    }
   ],
   "source": [
    "relations = relation_network(relation_pairs)\n",
    "print(relations.shape)\n",
    "\n",
    "relations = relations.view(-1, CLASS_NUM)\n",
    "print(relations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "one_hot_labels = torch.zeros(BATCH_NUM_PER_CLASS * CLASS_NUM, CLASS_NUM).to(device).scatter_(1, batch_labels.view(-1, 1), 1)\n",
    "loss = mse(relations, one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.44603148\n"
    }
   ],
   "source": [
    "print(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_encoder.zero_grad()\n",
    "relation_network.zero_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "torch.nn.utils.clip_grad_norm(feature_encoder.parameters(), 0.5)\n",
    "torch.nn.utils.clip_grad_norm(relation_network.parameters(), 0.5)\n",
    "\n",
    "feature_encoder_optim.step()\n",
    "relation_network_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.44603148\n"
    }
   ],
   "source": [
    "print(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "episode : 10, loss : 0.4160117506980896  \n",
    "episode : 20, loss : 0.363679438829422  \n",
    "episode : 30, loss : 0.28375476598739624  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test all iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training...\nepisode : 100, loss : 0.11256077140569687\nepisode : 200, loss : 0.08850272744894028\nepisode : 300, loss : 0.048718616366386414\nepisode : 400, loss : 0.04328080266714096\nepisode : 500, loss : 0.029594995081424713\nepisode : 600, loss : 0.02331722527742386\nepisode : 700, loss : 0.020142247900366783\nepisode : 800, loss : 0.010457553900778294\nepisode : 900, loss : 0.04053056985139847\nepisode : 1000, loss : 0.009467272087931633\nepisode : 1100, loss : 0.01664132997393608\nepisode : 1200, loss : 0.01804373972117901\nepisode : 1300, loss : 0.04295714199542999\nepisode : 1400, loss : 0.0034605690743774176\nepisode : 1500, loss : 0.024324458092451096\nepisode : 1600, loss : 0.04594869911670685\nepisode : 1700, loss : 0.008569257333874702\nepisode : 1800, loss : 0.019227461889386177\nepisode : 1900, loss : 0.016228288412094116\nepisode : 2000, loss : 0.0336613766849041\nepisode : 2100, loss : 0.021473877131938934\nepisode : 2200, loss : 0.010424191132187843\nepisode : 2300, loss : 0.026467030867934227\nepisode : 2400, loss : 0.008744304068386555\nepisode : 2500, loss : 0.015226173214614391\nTesting...\ntest accuracy :  0.9874799999999999\nsave networks for episode: 2499\nepisode : 2600, loss : 0.005925795994699001\nepisode : 2700, loss : 0.016182202845811844\nepisode : 2800, loss : 0.017698140814900398\nepisode : 2900, loss : 0.006595266051590443\nepisode : 3000, loss : 0.022145019844174385\nepisode : 3100, loss : 0.006660100072622299\nepisode : 3200, loss : 0.002343816449865699\nepisode : 3300, loss : 0.020243387669324875\nepisode : 3400, loss : 0.020531751215457916\nepisode : 3500, loss : 0.002329315058887005\nepisode : 3600, loss : 0.020355626940727234\nepisode : 3700, loss : 0.011597676202654839\nepisode : 3800, loss : 0.029842769727110863\nepisode : 3900, loss : 0.008658284321427345\nepisode : 4000, loss : 0.010042894631624222\nepisode : 4100, loss : 0.009166651405394077\nepisode : 4200, loss : 0.009466400370001793\nepisode : 4300, loss : 0.015761611983180046\nepisode : 4400, loss : 0.003316708840429783\nepisode : 4500, loss : 0.0027208945248275995\nepisode : 4600, loss : 0.03160163387656212\nepisode : 4700, loss : 0.0036252853460609913\nepisode : 4800, loss : 0.007174497935920954\nepisode : 4900, loss : 0.01012660376727581\nepisode : 5000, loss : 0.005739712156355381\nTesting...\ntest accuracy :  0.99172\nsave networks for episode: 4999\nepisode : 5100, loss : 0.009565582498908043\nepisode : 5200, loss : 0.009134401567280293\nepisode : 5300, loss : 0.05614818260073662\nepisode : 5400, loss : 0.003991786390542984\nepisode : 5500, loss : 0.003392392536625266\nepisode : 5600, loss : 0.007656164932996035\nepisode : 5700, loss : 0.0054007465951144695\nepisode : 5800, loss : 0.0017885518027469516\nepisode : 5900, loss : 0.0022281634155660868\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ac82c7f640fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/github-repos/relational-network/omniglot/task_generator_no_args.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "\n",
    "last_accuracy = 0.0\n",
    "\n",
    "for episode in range(EPISODE):\n",
    "    \n",
    "    #feature_encoder_scheduler.step(episode)\n",
    "    #relation_network_scheduler.step(episode)\n",
    "    \n",
    "    # * init dataset\n",
    "    # * sample_dataloader is to obtain previous samples for compare\n",
    "    # * batch_dataloader is to batch samples for training\n",
    "    degrees = random.choice([0, 90, 180, 270])\n",
    "    task = tg.OmniglotTask(metatrain_character_folders, CLASS_NUM, SAMPLE_NUM_PER_CLASS, BATCH_NUM_PER_CLASS)\n",
    "    sample_dataloader = tg.get_data_loader(task, num_per_class=SAMPLE_NUM_PER_CLASS, split=\"train\", shuffle=False, rotation=degrees)\n",
    "    batch_dataloader = tg.get_data_loader(task, num_per_class=BATCH_NUM_PER_CLASS, split=\"test\", shuffle=True, rotation=degrees)\n",
    "    \n",
    "    # * sample datas\n",
    "    # samples, sample_labels = sample_dataloader.__iter__().next()\n",
    "    # batches, batch_labels = batch_dataloader.__iter__().next()\n",
    "    \n",
    "    samples, sample_labels = next(iter(sample_dataloader))\n",
    "    batches, batch_labels = next(iter(batch_dataloader))\n",
    "    \n",
    "    samples, sample_labels = samples.to(device), sample_labels.to(device)\n",
    "    batches, batch_labels = batches.to(device), batch_labels.to(device)\n",
    "                    \n",
    "    # * calculates features\n",
    "    sample_features = feature_encoder(samples)\n",
    "    sample_features = sample_features.view(CLASS_NUM, SAMPLE_NUM_PER_CLASS, FEATURE_DIM, 5, 5)\n",
    "    sample_features = torch.sum(sample_features, 1).squeeze(1)\n",
    "    batch_features = feature_encoder(batches)\n",
    "    \n",
    "    # * calculate relations\n",
    "    # * each batch sample link to every samples to calculate relations\n",
    "    # * to form a 100 * 128 matrix for relation network\n",
    "    sample_features_ext = sample_features.unsqueeze(0).repeat(BATCH_NUM_PER_CLASS * CLASS_NUM, 1, 1, 1, 1)\n",
    "    batch_features_ext = batch_features.unsqueeze(0).repeat(CLASS_NUM, 1, 1, 1, 1)\n",
    "    batch_features_ext = torch.transpose(batch_features_ext, 0, 1)\n",
    "    \n",
    "    relation_pairs = torch.cat((sample_features_ext, batch_features_ext), 2).view(-1, FEATURE_DIM * 2, 5, 5)\n",
    "    relations = relation_network(relation_pairs).view(-1, CLASS_NUM)\n",
    "    \n",
    "    mse = nn.MSELoss()\n",
    "    one_hot_labels = torch.zeros(BATCH_NUM_PER_CLASS * CLASS_NUM, CLASS_NUM).to(device).scatter_(1, batch_labels.view(-1, 1), 1)\n",
    "    loss = mse(relations, one_hot_labels)\n",
    "    \n",
    "    feature_encoder.zero_grad()\n",
    "    relation_network.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm(feature_encoder.parameters(), 0.5)\n",
    "    torch.nn.utils.clip_grad_norm(relation_network.parameters(), 0.5)\n",
    "    \n",
    "    feature_encoder_optim.step()\n",
    "    relation_network_optim.step()\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"episode : {episode+1}, loss : {loss.cpu().detach().numpy()}\")\n",
    "        \n",
    "    if (episode + 1) % 2500 == 0:\n",
    "        print(\"Testing...\")\n",
    "        total_reward = 0\n",
    "        \n",
    "        for i in range(TEST_EPISODE):\n",
    "            degrees = random.choice([0, 90, 180, 270])\n",
    "            task = tg.OmniglotTask(metatest_character_folders, CLASS_NUM, SAMPLE_NUM_PER_CLASS, SAMPLE_NUM_PER_CLASS)\n",
    "            sample_dataloader = tg.get_data_loader(task, num_per_class=SAMPLE_NUM_PER_CLASS, split=\"train\", shuffle=False, rotation=degrees)\n",
    "            test_dataloader = tg.get_data_loader(task, num_per_class=SAMPLE_NUM_PER_CLASS, split=\"test\", shuffle=True, rotation=degrees)\n",
    "\n",
    "            # sample_images, sample_labels = sample_dataloader.__iter__().next()\n",
    "            # test_images, test_labels = test_dataloader.__iter__().next()            \n",
    "\n",
    "            sample_images, sample_labels = next(iter(sample_dataloader))\n",
    "            test_images, test_labels = next(iter(test_dataloader))\n",
    "\n",
    "            sample_images, sample_labels = sample_images.to(device), sample_labels.to(device)\n",
    "            test_images, test_labels = test_images.to(device), test_labels.to(device)\n",
    "                \n",
    "            # * calculate features\n",
    "            sample_features = feature_encoder(sample_images)\n",
    "            sample_features = sample_features.view(CLASS_NUM, SAMPLE_NUM_PER_CLASS, FEATURE_DIM, 5, 5)\n",
    "            sample_features = torch.sum(sample_features, 1).squeeze(1)\n",
    "            test_features = feature_encoder(test_images)\n",
    "            \n",
    "            # * calculate relations\n",
    "            # * each batch sample link to every samples to calculate relations\n",
    "            # * to form a 100x128 matrix for relation network\n",
    "            \n",
    "            sample_features_ext = sample_features.unsqueeze(0).repeat(SAMPLE_NUM_PER_CLASS * CLASS_NUM, 1, 1, 1, 1)\n",
    "            test_features_ext = test_features.unsqueeze(0).repeat(CLASS_NUM, 1, 1, 1, 1)\n",
    "            test_features_ext = torch.transpose(test_features_ext, 0, 1)         \n",
    "\n",
    "            relation_pairs = torch.cat((sample_features_ext, test_features_ext), 2).view(-1, FEATURE_DIM * 2, 5, 5)\n",
    "            relations = relation_network(relation_pairs).view(-1, CLASS_NUM)\n",
    "            \n",
    "            _, predict_labels = torch.max(relations.data, 1)\n",
    "            \n",
    "            rewards = [1 if predict_labels[j] == test_labels[j] else 0 for j in range(CLASS_NUM * SAMPLE_NUM_PER_CLASS)]\n",
    "            total_reward += np.sum(rewards)\n",
    "            \n",
    "        test_accuracy = total_reward/1.0/CLASS_NUM/SAMPLE_NUM_PER_CLASS/TEST_EPISODE\n",
    "        # total_reward / (1.0 * CLASS_NUM * SAMPLE_NUM_PER_CLASS * TEST_EPISODE)\n",
    "        \n",
    "        print(\"test accuracy : \", test_accuracy)\n",
    "        \n",
    "        if test_accuracy > last_accuracy:\n",
    "            # save networks\n",
    "            torch.save(\n",
    "                feature_encoder.state_dict(),\n",
    "                str(\"./models/omniglot_feature_encoder_\" + str(CLASS_NUM) + \"way_\" + str(SAMPLE_NUM_PER_CLASS) + \"shot.pkl\")\n",
    "            )\n",
    "\n",
    "            torch.save(\n",
    "                relation_network.state_dict(),\n",
    "                str(\"./models/omniglot_relation_network_\" + str(CLASS_NUM) + \"way_\" + str(SAMPLE_NUM_PER_CLASS) + \"shot.pkl\")\n",
    "            )\n",
    "\n",
    "            print(\"save networks for episode:\", episode)\n",
    "            last_accuracy = test_accuracy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38164bit66f7a8720ea44564891eb6b9b39e6c03",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}